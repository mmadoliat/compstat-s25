[
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven‚Äôt covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Comp-Stat",
    "section": "",
    "text": "About this site."
  },
  {
    "objectID": "slides/Chapter8.html#bootstrap-resampling-method",
    "href": "slides/Chapter8.html#bootstrap-resampling-method",
    "title": "Computational Statistics",
    "section": "Bootstrap Resampling Method",
    "text": "Bootstrap Resampling Method\n\nIntroduction to the bootstrap method\nExample: Bootstrap estimate of standard error\n\n\n# Bootstrap example: Estimate correlation between LSAT and GPA\nlibrary(bootstrap)    #for the law data\nprint(cor(law$LSAT, law$GPA))\n\n[1] 0.7763745\n\nprint(cor(law82$LSAT, law82$GPA))\n\n[1] 0.7599979\n\n# Set up the bootstrap\nB &lt;- 200            # number of replicates\nn &lt;- nrow(law)      # sample size\nR &lt;- numeric(B)     # storage for replicates\n# Bootstrap estimate of correlation\nfor (i in 1:B) {\n  idx &lt;- sample(1:n, size=n, replace=TRUE)\n  law_boot &lt;- law[idx, ]\n  R[i] &lt;- cor(law_boot$LSAT, law_boot$GPA)\n}\nmean(R)\n\n[1] 0.7802704\n\nsd(R)  # Bootstrap standard error\n\n[1] 0.1348875",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#visualizing-bootstrap-resamples",
    "href": "slides/Chapter8.html#visualizing-bootstrap-resamples",
    "title": "Computational Statistics",
    "section": "Visualizing Bootstrap Resamples",
    "text": "Visualizing Bootstrap Resamples\n\n# Plotting the bootstrap distribution\nhist(R, main=\"Bootstrap Distribution of Correlation\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#introduction-to-the-jackknife-method",
    "href": "slides/Chapter8.html#introduction-to-the-jackknife-method",
    "title": "Computational Statistics",
    "section": "Introduction to the Jackknife Method",
    "text": "Introduction to the Jackknife Method\n\nThe jackknife method for bias reduction\nExample: Jackknife estimate of the mean\n\n\n# Jackknife estimate of mean\nn &lt;- nrow(law)\ntheta_hat &lt;- mean(law$LSAT)\ntheta_jack &lt;- numeric(n)\n\n# Leave-one-out jackknife\nfor (i in 1:n) {\n  theta_jack[i] &lt;- mean(law$LSAT[-i])\n}\n\n# Jackknife estimate of bias\nbias_jack &lt;- (n - 1) * (mean(theta_jack) - theta_hat)\nbias_jack\n\n[1] 0",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#bootstrap-confidence-intervals",
    "href": "slides/Chapter8.html#bootstrap-confidence-intervals",
    "title": "Computational Statistics",
    "section": "Bootstrap Confidence Intervals",
    "text": "Bootstrap Confidence Intervals\n\nIntroduction to bootstrap confidence intervals\nExample: Bootstrap percentile confidence interval\n\n\n# Bootstrap confidence intervals\nalpha &lt;- 0.05\nci &lt;- quantile(R, probs = c(alpha/2, 1 - alpha/2))\nci  # Bootstrap confidence interval\n\n     2.5%     97.5% \n0.4422312 0.9701035",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#comparison-of-bootstrap-and-jackknife",
    "href": "slides/Chapter8.html#comparison-of-bootstrap-and-jackknife",
    "title": "Computational Statistics",
    "section": "Comparison of Bootstrap and Jackknife",
    "text": "Comparison of Bootstrap and Jackknife\n\nDiscuss the differences and use cases of bootstrap and jackknife\nRecap of methods and their benefits in reducing bias and estimating uncertainty",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#introduction-to-jackknife-after-bootstrap",
    "href": "slides/Chapter9.html#introduction-to-jackknife-after-bootstrap",
    "title": "Computational Statistics",
    "section": "Introduction to Jackknife-after-Bootstrap",
    "text": "Introduction to Jackknife-after-Bootstrap\n\nExplanation of the jackknife-after-bootstrap method\nUsed to estimate bias and standard errors after bootstrap\n\n\nlibrary(boot)\nlibrary(bootstrap)\nset.seed(1111)\n\n# Function to compute the patch ratio statistic\ntheta.boot &lt;- function(patch, i) {\n  y &lt;- patch[i, \"y\"]\n  z &lt;- patch[i, \"z\"]\n  mean(y) / mean(z)\n}\n\n# Bootstrap the patch dataset\nboot.out &lt;- boot(bootstrap::patch, statistic = theta.boot, R=2000)\nboot.out\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = bootstrap::patch, statistic = theta.boot, R = 2000)\n\n\nBootstrap Statistics :\n      original      bias    std. error\nt1* -0.0713061 0.008458915   0.1009777",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#applying-jackknife-after-bootstrap",
    "href": "slides/Chapter9.html#applying-jackknife-after-bootstrap",
    "title": "Computational Statistics",
    "section": "Applying Jackknife-after-Bootstrap",
    "text": "Applying Jackknife-after-Bootstrap\n\nExample: Using the jackknife-after-bootstrap method\nChecking the bootstrap array\n\n\n# Check the bootstrap array\nA &lt;- boot.array(boot.out)\nhead(A, 3)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    1    2    1    2    0    0    2\n[2,]    0    2    1    0    1    4    0    0\n[3,]    1    1    0    1    0    1    0    4\n\n# Proportion of cases that are not resampled\nmean(A[, 1] == 0)\n\n[1] 0.3455",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#visualizing-results",
    "href": "slides/Chapter9.html#visualizing-results",
    "title": "Computational Statistics",
    "section": "Visualizing Results",
    "text": "Visualizing Results\n\n# Plotting the bootstrap results\nplot(boot.out, index=1)",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#conclusion",
    "href": "slides/Chapter9.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of jackknife-after-bootstrap technique\nImportance of combining jackknife and bootstrap for robust estimates\nPractice: Apply jackknife-after-bootstrap to other datasets\n\n\n\n\n\nüîó tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#introduction-to-optimization",
    "href": "slides/Chapter14.html#introduction-to-optimization",
    "title": "Computational Statistics",
    "section": "Introduction to Optimization",
    "text": "Introduction to Optimization\n\nOverview of optimization methods\nExample: One-dimensional optimization using optimize\n\n\nf &lt;- function(x) log(x + log(x)) / log(1 + x)\n\n# Plotting the function\ncurve(f(x), from = 2, to = 15, ylab = \"f(x)\")\n\n# Finding the maximum\nres &lt;- optimize(f, lower = 4, upper = 8, maximum = TRUE)\nabline(v=res$maximum, col = \"red\", lwd = 2)\n\nres$maximum\n\n[1] 5.792299",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#introduction-to-mle",
    "href": "slides/Chapter14.html#introduction-to-mle",
    "title": "Computational Statistics",
    "section": "Introduction to MLE",
    "text": "Introduction to MLE\n\nIntroduction to Maximum Likelihood Estimation (MLE)\nExample: MLE for Gamma distribution\n\n\n# MLE for Gamma distribution\nm &lt;- 20000\nest &lt;- matrix(0, m, 2)\nn &lt;- 2000\nr &lt;- 5\nlambda &lt;- 2\n\nobj &lt;- function(lambda, xbar, logx.bar) {\n  r &lt;- length(xbar)\n  log(lambda) - lambda * mean(xbar) + logx.bar\n}",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#optimizing-mle",
    "href": "slides/Chapter14.html#optimizing-mle",
    "title": "Computational Statistics",
    "section": "Optimizing MLE",
    "text": "Optimizing MLE\n\n# Optimizing the MLE\nxbar &lt;- rnorm(n, mean = r/lambda, sd = 1)\nlogx.bar &lt;- mean(log(xbar))\nresult &lt;- optimize(obj, lower = 1, upper = 10, xbar = xbar, logx.bar = logx.bar, maximum = TRUE)\nresult\n\n$maximum\n[1] 9.999954\n\n$objective\n[1] NaN",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#conclusion",
    "href": "slides/Chapter14.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of one-dimensional optimization and MLE techniques\nPractice: Apply optimization to other statistical problems and models\n\n\n\n\n\nüîó tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#benchmarking-methods",
    "href": "slides/Chapter15.html#benchmarking-methods",
    "title": "Computational Statistics",
    "section": "Benchmarking Methods",
    "text": "Benchmarking Methods\n\nOverview of benchmarking in R\nExample: Benchmarking methods to generate a sequence\n\n\ns1 &lt;- 1:10\ns2 &lt;- seq(1, 10, 1)\ns3 &lt;- seq.int(1, 10, 1)\n\n# Benchmarking\nlibrary(microbenchmark)\nlibrary(ggplot2)\n\nn &lt;- 1000\nmb &lt;- microbenchmark(\n  seq(1, n, 1),\n  1:n,\n  times = 100\n)\nmb\n\nUnit: nanoseconds\n         expr   min      lq     mean  median      uq   max neval\n seq(1, n, 1) 17483 18083.5 19325.38 18475.0 18920.5 61856   100\n          1:n    91   120.0   198.90   185.5   240.5  1122   100\n\nautoplot(mb)",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#profiling-code-for-performance",
    "href": "slides/Chapter15.html#profiling-code-for-performance",
    "title": "Computational Statistics",
    "section": "Profiling Code for Performance",
    "text": "Profiling Code for Performance\n\nIntroduction to profiling in R\nExample: Profiling a function using profvis\n\n# Install profvis for profiling\nlibrary(profvis)\n\n# Example: Profile a sorting function\nprofvis({\n  x &lt;- rnorm(1e4)\n  y &lt;- sort(x)\n})",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#visualizing-profiling-results",
    "href": "slides/Chapter15.html#visualizing-profiling-results",
    "title": "Computational Statistics",
    "section": "Visualizing Profiling Results",
    "text": "Visualizing Profiling Results\n\n# profvis visual interface will show the profiling output",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#conclusion",
    "href": "slides/Chapter15.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of benchmarking and profiling in R\nPractice: Apply these techniques to optimize code in R\n\n\n\n\n\nüîó tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#introduction-to-monte-carlo-integration",
    "href": "slides/Chapter6.html#introduction-to-monte-carlo-integration",
    "title": "Computational Statistics",
    "section": "Introduction to Monte Carlo Integration",
    "text": "Introduction to Monte Carlo Integration\n\nBasic concepts of Monte Carlo integration\nExample: Integrating using uniform distribution\n\n\nm &lt;- 10000\nx &lt;- runif(m)\ntheta.hat &lt;- mean(exp(-x))\ntheta.hat\n\n[1] 0.6336453\n\n1 - exp(-1)\n\n[1] 0.6321206",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#example-monte-carlo-integration-with-bounded-intervals",
    "href": "slides/Chapter6.html#example-monte-carlo-integration-with-bounded-intervals",
    "title": "Computational Statistics",
    "section": "Example: Monte Carlo integration with bounded intervals",
    "text": "Example: Monte Carlo integration with bounded intervals\n\nm &lt;- 10000\nx &lt;- runif(m, min=2, max=4)\ntheta.hat &lt;- mean(exp(-x)) * 2\ntheta.hat\n\n[1] 0.1182733\n\nexp(-2) - exp(-4)\n\n[1] 0.1170196",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#unbounded-intervals",
    "href": "slides/Chapter6.html#unbounded-intervals",
    "title": "Computational Statistics",
    "section": "Unbounded Intervals",
    "text": "Unbounded Intervals\n\nHandling unbounded intervals in Monte Carlo integration\n\n\n# Plotting a function over an unbounded interval\nx &lt;- seq(.1, 2.5, length.out = 100)\ny &lt;- exp(-x^2 / 2)\nplot(x, y, type=\"l\", main=\"Function Plot for Unbounded Interval\", col=\"blue\")",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#introduction-to-importance-sampling",
    "href": "slides/Chapter6.html#introduction-to-importance-sampling",
    "title": "Computational Statistics",
    "section": "Introduction to Importance Sampling",
    "text": "Introduction to Importance Sampling\n\nImportance sampling and its applications\nExample: Applying importance sampling\n\n\n# Example of importance sampling\ng &lt;- function(x) exp(-x^2 / 2)\nf &lt;- function(x) dnorm(x, mean = 1)\nx &lt;- rnorm(10000, mean = 1)\ntheta.hat &lt;- mean(g(x) / f(x))\ntheta.hat\n\n[1] 2.471867",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#visualizing-importance-sampling",
    "href": "slides/Chapter6.html#visualizing-importance-sampling",
    "title": "Computational Statistics",
    "section": "Visualizing Importance Sampling",
    "text": "Visualizing Importance Sampling\n\n# Compare the original function with the importance sampling function\ncurve(g, from = -3, to = 3, col = \"blue\", lwd = 2, main=\"Importance Sampling: g(x) vs f(x)\")\ncurve(f, add = TRUE, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topright\", legend=c(\"g(x)\", \"f(x)\"), col=c(\"blue\", \"red\"), lty=c(1, 2), lwd=2)",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#control-variates",
    "href": "slides/Chapter6.html#control-variates",
    "title": "Computational Statistics",
    "section": "Control Variates",
    "text": "Control Variates\n\nExplanation of control variates and how they reduce variance\nExample: Using control variates in Monte Carlo integration\n\n\n# Control variates example\nm &lt;- 10000\nx &lt;- runif(m)\ntheta.hat &lt;- mean(exp(-x)) - (mean(x) - 0.5)\ntheta.hat\n\n[1] 0.6317725",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#antithetic-variables",
    "href": "slides/Chapter6.html#antithetic-variables",
    "title": "Computational Statistics",
    "section": "Antithetic Variables",
    "text": "Antithetic Variables\n\nExplanation of antithetic variables and how they reduce variance\nExample: Applying antithetic variables\n\n\n# Antithetic variables example\nu &lt;- runif(5000)\ntheta.hat &lt;- mean((exp(-u) + exp(-(1 - u))) / 2)\ntheta.hat\n\n[1] 0.6319557\n\n1 - exp(-1)\n\n[1] 0.6321206",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#basic-monte-carlo-estimation",
    "href": "slides/Chapter7.html#basic-monte-carlo-estimation",
    "title": "Computational Statistics",
    "section": "Basic Monte Carlo Estimation",
    "text": "Basic Monte Carlo Estimation\n\nIntroduction to Monte Carlo estimation\nExample: Estimating the expected difference of two normal variables\n\n\nm &lt;- 1000\ng &lt;- numeric(m)\nfor (i in 1:m) {\n    x &lt;- rnorm(2)\n    g[i] &lt;- abs(x[1] - x[2])\n}\nest &lt;- mean(g)\nest\n\n[1] 1.153167",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#visualizing-monte-carlo-simulations",
    "href": "slides/Chapter7.html#visualizing-monte-carlo-simulations",
    "title": "Computational Statistics",
    "section": "Visualizing Monte Carlo Simulations",
    "text": "Visualizing Monte Carlo Simulations\n\n# Histogram of the simulated differences\nhist(g, main=\"Histogram of Differences (Monte Carlo)\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#estimating-the-mean-squared-error-mse",
    "href": "slides/Chapter7.html#estimating-the-mean-squared-error-mse",
    "title": "Computational Statistics",
    "section": "Estimating the Mean Squared Error (MSE)",
    "text": "Estimating the Mean Squared Error (MSE)\n\nMonte Carlo estimation of MSE for trimmed means\n\n\nn &lt;- 20\nm &lt;- 1000\nmean_trim &lt;- numeric(m)\nfor (i in 1:m) {\n    x &lt;- rnorm(n)\n    mean_trim[i] &lt;- mean(x, trim = 0.1)\n}\nmse &lt;- mean((mean_trim - 0)^2)\nmse\n\n[1] 0.05611406",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#visualizing-mse-simulations",
    "href": "slides/Chapter7.html#visualizing-mse-simulations",
    "title": "Computational Statistics",
    "section": "Visualizing MSE Simulations",
    "text": "Visualizing MSE Simulations\n\n# Histogram of the trimmed means\nhist(mean_trim, main=\"Trimmed Means (Monte Carlo MSE)\", col=\"lightgreen\")",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#additional-monte-carlo-techniques",
    "href": "slides/Chapter7.html#additional-monte-carlo-techniques",
    "title": "Computational Statistics",
    "section": "Additional Monte Carlo Techniques",
    "text": "Additional Monte Carlo Techniques\n\nExploring advanced Monte Carlo techniques for inference\nExample: Hypothesis testing with Monte Carlo methods\n\n\n# Monte Carlo test example: Hypothesis testing\nn &lt;- 50\nm &lt;- 1000\ntest_stat &lt;- numeric(m)\nfor (i in 1:m) {\n    x &lt;- rnorm(n)\n    test_stat[i] &lt;- mean(x)\n}\np_value &lt;- mean(test_stat &gt;= 1.96)\np_value\n\n[1] 0",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#conclusion",
    "href": "slides/Chapter7.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of Monte Carlo methods in inference, MSE estimation, and hypothesis testing\nPractice: Apply these methods to other inferential problems\n\n\n\n\n\nüîó tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "hw/HW 1.html",
    "href": "hw/HW 1.html",
    "title": "Computational Statistics - HW 1",
    "section": "",
    "text": "Questions 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, and 1.8\n\n7 out of 8 Question (for Undergraduates)\nAll Questions (for Graduates)",
    "crumbs": [
      "HW",
      "HW 1"
    ]
  },
  {
    "objectID": "hw/HW 1.html#from-the-textbook",
    "href": "hw/HW 1.html#from-the-textbook",
    "title": "Computational Statistics - HW 1",
    "section": "",
    "text": "Questions 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, and 1.8\n\n7 out of 8 Question (for Undergraduates)\nAll Questions (for Graduates)",
    "crumbs": [
      "HW",
      "HW 1"
    ]
  },
  {
    "objectID": "hw/HW 1.html#extra-question",
    "href": "hw/HW 1.html#extra-question",
    "title": "Computational Statistics - HW 1",
    "section": "Extra Question",
    "text": "Extra Question\nFollow the example we had in the classroom, find the \\(E(X)\\) and \\(Var(X)\\) based on (i) equations (2.1) and (2.2), and (ii) simulation for the following cases. Does (i) and (ii) give you the same result?\n(a) \\(X\\sim\\chi^2(\\nu=N+1)\\), where \\(N\\sim Poisson(\\lambda)\\).\n(b) (Only for Graduate Students) \\(X\\sim Binomial(n=Y, p=P)\\), where \\(Y\\sim Poisson(\\lambda)\\) and \\(P\\sim Beta(\\alpha, \\beta)\\).",
    "crumbs": [
      "HW",
      "HW 1"
    ]
  },
  {
    "objectID": "hw/HW 2.html",
    "href": "hw/HW 2.html",
    "title": "Computational Statistics - HW 2",
    "section": "",
    "text": "Questions 3.2, 3.3, 3.7, 3.8, 3.11, 3.12, 3.13, and 3.14\n\n6 out of 8 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 2.html#from-textbook",
    "href": "hw/HW 2.html#from-textbook",
    "title": "Computational Statistics - HW 2",
    "section": "",
    "text": "Questions 3.2, 3.3, 3.7, 3.8, 3.11, 3.12, 3.13, and 3.14\n\n6 out of 8 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 5.html",
    "href": "hw/HW 5.html",
    "title": "Computational Statistics - HW 5",
    "section": "",
    "text": "Questions 7.1, 7.2, 7.3, 7.5, 7.6, 7.7, and 7.8 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 5.html#from-textbook",
    "href": "hw/HW 5.html#from-textbook",
    "title": "Computational Statistics - HW 5",
    "section": "",
    "text": "Questions 7.1, 7.2, 7.3, 7.5, 7.6, 7.7, and 7.8 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 3.html",
    "href": "hw/HW 3.html",
    "title": "Computational Statistics - HW 3",
    "section": "",
    "text": "Questions 5.1, 5.2, 5.3, 5.4, 5.5, 5.6 from the Textbook.\n\n5 out of 6 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 3.html#from-textbook",
    "href": "hw/HW 3.html#from-textbook",
    "title": "Computational Statistics - HW 3",
    "section": "",
    "text": "Questions 5.1, 5.2, 5.3, 5.4, 5.5, 5.6 from the Textbook.\n\n5 out of 6 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Homework\nStat Calculator\nDistribution Calculator\nStat Tables\nüîó on D2L\nüîó StatCalc(JAMM)\nüîó DistCalc\nüîó StatTables\n\n\nTexbooks\nüîó Computational Statistics in Python\nüîó R Package\nüîó Advanced R by Hadley Wickham\nüîó Quarto\nüîó R Markdown\nüîó ggplot2: Elegant Graphics for Data Analysis\nüîó Fundamentals of Data Visualization\nüîó Data Visualization: A Practical Introduction\nüîó R for Data Science\n\n\nSome Package documentation\nüîó ggplot2: ggplot2.tidyverse.org\nüîó dplyr: dplyr.tidyverse.org\nüîó tidyr: tidyr.tidyverse.org\nüîó forcats: forcats.tidyverse.org\nüîó stringr: stringr.tidyverse.org\nüîó lubridate: lubridate.tidyverse.org\nüîó readr: readr.tidyverse.org",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics - Spring 2025",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nTOPIC\nMATERIALS\nCODE\nDUE\n\n\n\n\n1\nTue, Jan 14\nWhat to cover in this class?\nüîóSyllabus\n\n\n\n\n\n\n\n\nThu, Jan 16\nIntroduction to Statistical Computing üé• Lecture 2\nüìöChapt 1-A\nRüë©‚Äçüíª\n\n\n\n\n\n\nFri, Jan 17\nReview Probability  and Statistics\nüíªReview Probability  üìñIntroduction to Statistics ebook\n\n\nüìù HW 0 or  Learn R in R\n\n\n2\nTue, Jan 21\n‚õà Sever Weather - No Class\n\n\n\n\n\n\n\n\n\n\nThu, Jan 23\nProbability and Statistics Review üé• Lecture 3\nüìöChapt 2-A\nRüë©‚Äçüíª\n\n\n\n\n3\nTue, Jan 28\nRegression üé• Lecture 4\nüíªRegression\nR Shinyüë©‚Äçüíª\n\n\n\n\n\n\nThu, Jan 30\nRegularization in Regression  Method of Estimations üé• Lecture 5\nüíªRidge and Lasso  üìöüíªChapt 2-B\n\n\n\n\n\n\n\n\nFri, Jan 31\n\n\n\n\nD2L‚úçÔ∏è\nüìù HW 1 at 11:50 pm Solution\n\n\n4\nTue, Feb 4\nMethods ofGenerating Random Vars. üé• Lecture 6\nüìöChapt 3-A\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Feb 6\nMore on Generating RV‚Äôs üé• Lecture 7\nüìöChapt 3-B\n\n\n\n\n\n\n5\nTue, Feb 11\nK-means Clustering üé• Lecture 8\nüíªClustering-A\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Feb 13\nHirarchical Clustering üé• Lecture 9\nüíªClustering-B\n\n\n\n\n\n\n\n\nFri, Feb 14\n\n\n\n\nD2L‚úçÔ∏è\nüìù HW 2 at 11:50 pm Solution\n\n\n6\nTue, Feb 18\nVizualization üé• Lecture 10\nüìöChapt 5\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Feb 20\nMonte Carlo Integration üé• Lecture 11\nüìöChapt 6-A\nRüë©‚Äçüíª\n\n\n\n\n7\nTue, Feb 25\nVariance Reduction üé• Lecture 12\nüìöChapt 6-B\n\n\n\n\n\n\n\n\nThu, Feb 27\nImportance Sampling and Var. Red. üé• Lecture 13  Developing an R package\nüìöChapt 6-C  üìñR package\n\n\n\n\n\n\n\n\nFri, Feb 28\n\n\n\n\nD2L‚úçÔ∏è\nüìù HW 3 at 11:50 pm Solution\n\n\n8\nTue, Mar 4\nDeveloping an Interactive Shiny App üé• Lecture 14\nüìñShiny\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Mar 6\nObject Oriented Programming in R üé• Lecture 15\nüìñOOP\nRüë©‚Äçüíª\n\n\n\n\n\n\nFri, Mar 7\n\n\n\n\nD2L‚úçÔ∏è\nüìù HW 4 at 11:50 pm Solution\n\n\n\n\nTue, Mar 11\nüå¥ Spring Break\n\n\n\n\n\n\n\n\n\n\nThu, Mar 13\nüå¥ Spring Break\n\n\n\n\n\n\n\n\n9\nTue, Mar 18\nMonte Carlo Methods üé• Lecture 16\nüìöChapt 7-A\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Mar 20\nMore on Monte Carlo Methods üé• Lecture 17\nüìöChapt 7-B  üìöüíªChapt 7-C\n\n\n\n\n\n\n10\nTue, Mar 25\nBootstrap üé• Lecture 18\nüìöChapt 8-A\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Mar 27\nMore on Bootstrap and Jackknife üé• Lecture 19\nüìëChapt 8-B  üñ•Ô∏èChapt 8-C  üìöChapt 9\nRüë©‚Äçüíª\n\n\n\n\n\n\nFri, Mar 28\n\n\n\n\nD2L‚úçÔ∏è\nüìù HW 5 at 11:50 pm Solution\n\n\n11\nTue, Apr 1\nDensity Estimation üé• Lecture 20\nüìöChapt 12\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Apr 3\nNumerical Methods in R üé• Lecture 21\nüìöChapt 13-A\nRüë©‚Äçüíª\n\n\n\n\n\n\nFri, Apr 4\n\n\n\n\nD2L‚úçÔ∏è\nüìù HW 6 at 11:50 pm Solution\n\n\n12\nTue, Apr 8\nMore on Numerical Methods üé• Lecture 22  Optimization in R\nüìöChapt 13-B\n\n\n\n\n\n\n\n\nThu, Apr 10\nMore on Optimization üé• Lecture 23\nüìöChapt 14-A\nRüë©‚Äçüíª\n\n\n\n\n\n\nFri, Apr 11\n\n\n\n\n[D2L‚úçÔ∏è]\nüìù [HW 7] at 11:50 pm\n\n\n13\nTue, Apr 15\nExpectation-Maximization Algorithm üé• Lecture 24\nüìëChapt 14-B\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Apr 17\nüê∞ Easter Break\n\n\n\n\n\n\n\n\n14\nTue, Apr 22\nProgramming Topics\nüìöChapt 15\nRüë©‚Äçüíª\n\n\n\n\n\n\nThu, Apr 24\nDeep Learning in R\nüíªDLiR-A\nRüë©‚Äçüíª\n\n\n\n\n15\nTue, Apr 29\nFully Connected Networks (FCN)\nüíªDLiR-B\n\n\n\n\n\n\n\n\nThu, May 1\nConvolutional Nueral Nets (CNN)\nüíªDLiR-C\n\n\n\n\n\n\n16\nMon, May 5\nFinal Project\nüìÉPresentations : 10:30 am - 12:30 pm",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "hw/HW 4.html",
    "href": "hw/HW 4.html",
    "title": "Computational Statistics - HW 4",
    "section": "",
    "text": "Questions 6.1, 6.2, 6.3, 6.6, 6.7, 6.9, and 6.12 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 4.html#from-textbook",
    "href": "hw/HW 4.html#from-textbook",
    "title": "Computational Statistics - HW 4",
    "section": "",
    "text": "Questions 6.1, 6.2, 6.3, 6.6, 6.7, 6.9, and 6.12 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 7.html",
    "href": "hw/HW 7.html",
    "title": "Computational Statistics - HW 7",
    "section": "",
    "text": "Questions 8.2, 8.10, 8.11, 9.1, 9.4 from the Textbook, and ‚ÄúCourse Evaluation‚Äù.\n\n4 out of 6 Questions (for Undergraduate students)\n5 out of 6 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 7.html#from-textbook",
    "href": "hw/HW 7.html#from-textbook",
    "title": "Computational Statistics - HW 7",
    "section": "",
    "text": "Questions 8.2, 8.10, 8.11, 9.1, 9.4 from the Textbook, and ‚ÄúCourse Evaluation‚Äù.\n\n4 out of 6 Questions (for Undergraduate students)\n5 out of 6 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 6.html",
    "href": "hw/HW 6.html",
    "title": "Computational Statistics - HW 6",
    "section": "",
    "text": "Questions 8.1, 8.4, 8.5, 8.6, 8.7, 8.8 and 8.9 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 6.html#from-textbook",
    "href": "hw/HW 6.html#from-textbook",
    "title": "Computational Statistics - HW 6",
    "section": "",
    "text": "Questions 8.1, 8.4, 8.5, 8.6, 8.7, 8.8 and 8.9 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "",
    "text": "Course Title: MATH 4750/MSSC 5750: Computational Statistics\nMeeting Time: TuTh 2:00pm - 3:15pm\nLocation: David Straz Nursing 402 (Microsoft Teams)\nWebsite: http://tinyurl.com/Comp-Stat",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "",
    "text": "Course Title: MATH 4750/MSSC 5750: Computational Statistics\nMeeting Time: TuTh 2:00pm - 3:15pm\nLocation: David Straz Nursing 402 (Microsoft Teams)\nWebsite: http://tinyurl.com/Comp-Stat",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#instructor-details",
    "href": "course-syllabus.html#instructor-details",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Instructor Details",
    "text": "Instructor Details\n\nName: Mehdi Maadooliat, Ph.D.\nOffice: CU 351\nOffice Hours: TuTh 12:25 - 1:55pm in CU 351 or by e-mail",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Course Description",
    "text": "Course Description\nComputational statistics and statistical computing are two areas that employ computational, graphical, and numerical approaches to solve statistical problems. Competent statisticians must not just run existing programs but understand the principles behind them. This class introduces statistically-oriented programming targeted at statistics majors, with no extensive programming background assumed.\nThis course covers traditional core material of computational statistics with an emphasis on using R through an example-based approach.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-outcomes",
    "href": "course-syllabus.html#learning-outcomes",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the course, students will:\n\nUnderstand core programming concepts, including data structures, functions, iteration, debugging, logical design, and abstraction.\nWrite maintainable code, debug, and test code for correctness.\nSet up and run stochastic simulations in parallel.\nFit basic statistical models and assess the results.\nWork with and filter large datasets.\nComment and organize code effectively.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCOSC 1010\nMATH 3100\nMATH 4700 or MATH 4780",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Textbooks",
    "text": "Textbooks\n\nMain: Statistical Computing with R, 2nd edition by Maria L. Rizzo. Chapman and Hall/CRC, 2019. ISBN: 1466553324.\nSupplementary: Computational Statistics, 2nd Edition by Geof H. Givens and Jennifer A. Hoeting. Wiley, 2012. ISBN: 9780470533314.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-breakdown",
    "href": "course-syllabus.html#grading-breakdown",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Grading Breakdown",
    "text": "Grading Breakdown\n\nClass Participation: 10%\nHomework: 30%\nTeam Project: 30%\nFinal Exam or HW/Project (Instructor will decide): 30%\n\n\nGrading Scale\n\n\n\nGrade\nRange\n\n\n\n\nA\n95 - 100%\n\n\nA-\n89 - 94.99%\n\n\nB+\n84 - 88.99%\n\n\nB\n78 - 83.99%\n\n\nB-\n73 - 77.99%\n\n\nC+\n67 - 72.99%\n\n\nC\n62 - 66.99%\n\n\nC-\n56 - 61.99%\n\n\nD+\n51 - 55.99%\n\n\nD\n45 - 50.99%\n\n\nF\n&lt; 44.99%\n\n\n\nNotes:\n\nFor students in MSSC 5750, there will be extra questions in Homework, and higher expectation for Project.\nNo late homework will be accepted, and no make-up work will be allowed unless in the case of an emergency. Submit incomplete work if necessary, but ensure scanned PDFs are legible before submission.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#make-up-policy",
    "href": "course-syllabus.html#make-up-policy",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Make-up Policy",
    "text": "Make-up Policy\nNo make-up exams or homework unless there is an emergency.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#attendance",
    "href": "course-syllabus.html#attendance",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Attendance",
    "text": "Attendance\nAttendance is required and subject to the College of Arts and Sciences policy.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-honesty",
    "href": "course-syllabus.html#academic-honesty",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Academic Honesty",
    "text": "Academic Honesty\nStudents are expected to follow the University‚Äôs policy on academic honesty as outlined in the Bulletin.\nTL;DR: Don‚Äôt cheat!\nPlease abide by the following as you work on assignments in this course:\n\nCollaboration: Only work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must also be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to questions (including any code) with anyone other than myself.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nOn individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\nOnline resources: I am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course‚Äôs policy is that you may make use of any online resources (e.g., StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of generative artificial intelligence (AI): You should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n‚úÖ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n‚ùå AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you.\n\nYou are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask the instructor.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-topics",
    "href": "course-syllabus.html#course-topics",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Course Topics",
    "text": "Course Topics\n\nManaging Input/Output in R\nData structures in R\nData transformations: strings, factors, dates\nStatistical summaries and basic tests in R\nGraphics in R\nMonte Carlo Integration & Variance Reduction\nBootstrap and Jackknife\nDensity Estimation\nRegularization:\n\nRidge and Lasso\nSmoothing\n\nDimension Reduction and Clustering\n\nK-Means and Hierarchical Clustering\nPrincipal Component Analysis (PCA)\n\nNumerical Methods in R\n\nOptimization\nExpectation-Maximization (EM) Algorithm\n\nAdvanced R Programming\n\nIntroduction to Parallel Computing\nObject Oriented Programming in R\nDeveloping an R Package\nDeveloping an Interactive Shiny App\nFunctions and loops in C and using C in R\n\nDeep Learning in R\n\nBasics of Neural Network\n\nImplementation using TensorFlow and Keras\n\nFully Connected Network (FCN)\nConvolutional Neural Networks (CNN)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Important dates",
    "text": "Important dates\n\nMonday, January 13: Classes begin\nTuesday, January 21: Drop/add ends\nSaturday - Sunday, March 8 - 16: Spring Break\nFriday, April 11: Last day to withdraw with W\nThursday - Monday, April 17 - 21: Easter Break\nSaturday, May 3: Classes end\nMonday, May 5, 10:30 am - 12:30 pm: Presentations\n\nFor more important dates, see the full MU Academic Calendar.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-note",
    "href": "course-syllabus.html#important-note",
    "title": "Syllabus - Computational Statistics - Spring 2025",
    "section": "Important Note",
    "text": "Important Note\nThe syllabus may be modified throughout the course. Any substantial modifications will result in a reissued syllabus.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/Chapter1.html#probability-distributions",
    "href": "slides/Chapter1.html#probability-distributions",
    "title": "Computational Statistics",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\ndnorm(), pnorm(), qnorm()\nExploring statistical tests in R",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#statistical-tests",
    "href": "slides/Chapter1.html#statistical-tests",
    "title": "Computational Statistics",
    "section": "Statistical Tests",
    "text": "Statistical Tests\n\nt.test(), chisq.test()",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#defining-functions",
    "href": "slides/Chapter1.html#defining-functions",
    "title": "Computational Statistics",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nfunction(arglist) expr\nReturn values and default arguments\n\n\nsumdice &lt;- function(n) {\n  k &lt;- sample(1:6, size=n, replace=TRUE)\n  return(sum(k))\n}\nsumdice(2)\n\n[1] 8",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#data-structures-in-r",
    "href": "slides/Chapter1.html#data-structures-in-r",
    "title": "Computational Statistics",
    "section": "Data Structures in R",
    "text": "Data Structures in R\n\nArrays, matrices, and data frames\n\n\n# Creating vectors and matrices\nx &lt;- c(1, 2, 3, 4)\nmatrix_x &lt;- matrix(x, nrow=2, ncol=2)\nprint(matrix_x)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\nExample: Iris data set\n\n\ndata(iris)\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#basic-plots",
    "href": "slides/Chapter1.html#basic-plots",
    "title": "Computational Statistics",
    "section": "Basic Plots",
    "text": "Basic Plots\n\nplot(), hist(), boxplot()\n\n\nCodePlot\n\n\n\nplot(iris);\n# boxplot(iris);\n# hist(iris[,1])",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#introduction-to-ggplot2",
    "href": "slides/Chapter1.html#introduction-to-ggplot2",
    "title": "Computational Statistics",
    "section": "Introduction to ggplot2",
    "text": "Introduction to ggplot2\n\nVisualizing using ggplot2\n\n\nCodePlot\n\n\n\nlibrary(ggplot2)\nggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE, color=\"blue\") + \n  theme_minimal() + \n  labs(title=\"Sepal vs Petal Length in Iris Dataset\", x=\"Sepal Length\", y=\"Petal Length\")",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#managing-files",
    "href": "slides/Chapter1.html#managing-files",
    "title": "Computational Statistics",
    "section": "Managing Files",
    "text": "Managing Files\n\nWorking directories and file input/output\nUsing scripts and automation",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#dynamic-documents",
    "href": "slides/Chapter1.html#dynamic-documents",
    "title": "Computational Statistics",
    "section": "Dynamic Documents",
    "text": "Dynamic Documents\n\nCreating reports with R Markdown\nIntroduction to knitr package",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter12.html#histogram-density-estimates",
    "href": "slides/Chapter12.html#histogram-density-estimates",
    "title": "Computational Statistics",
    "section": "Histogram Density Estimates",
    "text": "Histogram Density Estimates\n\nIntroduction to density estimation using histograms\nExample: Histogram density estimates using Sturges‚Äô Rule\n\n\nset.seed(12345)\nn &lt;- 25\nx &lt;- rnorm(n)\n\n# Calculate breaks according to Sturges' Rule\nnclass &lt;- ceiling(1 + log2(n))\ncwidth &lt;- diff(range(x) / nclass)\nbreaks &lt;- min(x) + cwidth * 0:nclass\n\n# Default histogram\nh.default &lt;- hist(x, freq = FALSE, xlab = \"default\", main = \"Histogram: Default\")",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter12.html#kernel-density-estimation",
    "href": "slides/Chapter12.html#kernel-density-estimation",
    "title": "Computational Statistics",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\n\nIntroduction to kernel density estimation (KDE)\nExample: Applying KDE to data\n\n\n# Kernel density estimate\ndens &lt;- density(x)\nplot(dens, main = \"Kernel Density Estimate\", col = \"blue\", lwd = 2)",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter12.html#visualizing-kernel-density",
    "href": "slides/Chapter12.html#visualizing-kernel-density",
    "title": "Computational Statistics",
    "section": "Visualizing Kernel Density",
    "text": "Visualizing Kernel Density\n\n# Overlay histogram and KDE\nhist(x, freq = FALSE, col = \"lightgray\", border = \"white\", main = \"Histogram with KDE\")\nlines(dens, col = \"red\", lwd = 2)",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter12.html#conclusion",
    "href": "slides/Chapter12.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of histogram density estimation and kernel density estimation\nPractice: Apply density estimation techniques to other datasets\n\n\n\n\n\nüîó tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#sampling-from-a-finite-population",
    "href": "slides/Chapter3.html#sampling-from-a-finite-population",
    "title": "Computational Statistics",
    "section": "Sampling from a Finite Population",
    "text": "Sampling from a Finite Population\n\nIntroduction to finite population sampling\nExample: Tossing coins, choosing lottery numbers\n\n\n# Sampling examples\nsample(0:1, size = 10, replace = TRUE)  # Tossing coins\n\n [1] 1 0 0 1 1 1 1 1 1 0\n\nsample(1:100, size = 6, replace = FALSE)  # Choosing lottery numbers\n\n[1] 27 44 30 14 23 83\n\nsample(letters)  # Permutation of letters\n\n [1] \"s\" \"n\" \"u\" \"g\" \"c\" \"v\" \"i\" \"e\" \"d\" \"f\" \"x\" \"o\" \"b\" \"p\" \"z\" \"r\" \"y\" \"l\" \"q\"\n[20] \"j\" \"k\" \"h\" \"a\" \"w\" \"t\" \"m\"",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#multinomial-distribution",
    "href": "slides/Chapter3.html#multinomial-distribution",
    "title": "Computational Statistics",
    "section": "Multinomial Distribution",
    "text": "Multinomial Distribution\n\nIntroduction to multinomial distribution\nExample: Sampling from a multinomial distribution\n\n\n# Sample from multinomial distribution\nx &lt;- sample(1:3, size = 100, replace = TRUE, prob = c(.2, .3, .5))\ntable(x)\n\nx\n 1  2  3 \n15 25 60",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#continuous-case",
    "href": "slides/Chapter3.html#continuous-case",
    "title": "Computational Statistics",
    "section": "Continuous Case",
    "text": "Continuous Case\n\nExplanation of the inverse transform method for continuous distributions\nExample: Simulating exponential random variables\n\n\n# Inverse transform method for exponential distribution\nn &lt;- 1000\nU &lt;- runif(n)\nX &lt;- -log(1 - U)\nhist(X, main=\"Exponential Distribution via Inverse Transform\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#discrete-case",
    "href": "slides/Chapter3.html#discrete-case",
    "title": "Computational Statistics",
    "section": "Discrete Case",
    "text": "Discrete Case\n\nExample of applying the inverse transform method to discrete random variables\n\n\n# Inverse transform method for geometric distribution\np &lt;- 0.5\nX_geom &lt;- ceiling(log(1 - runif(n)) / log(1 - p))\nhist(X_geom, main=\"Geometric Distribution via Inverse Transform\", col=\"lightgreen\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#introduction-to-acceptance-rejection-method",
    "href": "slides/Chapter3.html#introduction-to-acceptance-rejection-method",
    "title": "Computational Statistics",
    "section": "Introduction to Acceptance-Rejection Method",
    "text": "Introduction to Acceptance-Rejection Method\n\nExplanation of the acceptance-rejection method\nExample: Sampling from a target distribution\n\n\n# Acceptance-rejection method example\ntarget &lt;- function(x) { ifelse(x &gt; 0, exp(-x), 0) }\nproposal &lt;- function(x) { dnorm(x, mean = 2, sd = 1) }\n\nX &lt;- rnorm(1000, mean = 2, sd = 1)\naccept &lt;- runif(1000) &lt; target(X) / (1.5 * proposal(X))\n\nhist(X[accept], main=\"Accepted Samples from Target Distribution\", col=\"lightcoral\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#transformation-of-random-variables",
    "href": "slides/Chapter3.html#transformation-of-random-variables",
    "title": "Computational Statistics",
    "section": "Transformation of Random Variables",
    "text": "Transformation of Random Variables\n\nIntroduction to transformation methods\nExample: Box-Muller transform for generating normal random variables\n\n\n# Box-Muller transform\nn &lt;- 1000\nU1 &lt;- runif(n)\nU2 &lt;- runif(n)\nZ1 &lt;- sqrt(-2 * log(U1)) * cos(2 * pi * U2)\nZ2 &lt;- sqrt(-2 * log(U1)) * sin(2 * pi * U2)\nhist(Z1, main=\"Normal Distribution via Box-Muller\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#sums-and-mixtures",
    "href": "slides/Chapter3.html#sums-and-mixtures",
    "title": "Computational Statistics",
    "section": "Sums and Mixtures",
    "text": "Sums and Mixtures\n\nExplanation of sums and mixtures of random variables\nExample: Mixture of normals\n\n\n# Mixture of normals\nlibrary(MASS)\nmu1 &lt;- 0; mu2 &lt;- 3; sigma1 &lt;- 1; sigma2 &lt;- 2\np &lt;- 0.3\nX &lt;- ifelse(runif(n) &lt; p, rnorm(n, mu1, sigma1), rnorm(n, mu2, sigma2))\nhist(X, main=\"Mixture of Normal Distributions\", col=\"lightgreen\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#random-variables-and-distributions",
    "href": "slides/Chapter2.html#random-variables-and-distributions",
    "title": "Computational Statistics",
    "section": "Random Variables and Distributions",
    "text": "Random Variables and Distributions\n\nOverview of random variables\nDiscrete vs Continuous random variables\nImportant distributions (Binomial, Normal, Chi-Square)\n\n\n# Simulating random variables\nN &lt;- 100; mu &lt;- 5; sig &lt;- 2\nY &lt;- rnorm(N, mean = mu, sd = sig)\nhist(Y, main=\"Histogram of Normal Distribution\", xlab=\"Y values\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#binomial-distribution",
    "href": "slides/Chapter2.html#binomial-distribution",
    "title": "Computational Statistics",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nrbinom(), dbinom(), pbinom()\nExample: Generating binomial random variables\n\n\nrbinom(10, size=10, prob=0.5)\n\n [1] 5 4 6 2 4 5 3 7 5 4",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#poisson-distribution",
    "href": "slides/Chapter2.html#poisson-distribution",
    "title": "Computational Statistics",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nrpois(), dpois(), ppois()\nExample: Simulating Poisson-distributed data\n\n\nrpois(10, lambda=2)\n\n [1] 1 1 1 2 1 3 2 2 1 1",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#normal-distribution",
    "href": "slides/Chapter2.html#normal-distribution",
    "title": "Computational Statistics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nrnorm(), dnorm(), pnorm()\nExample: Generating normal random variables\n\n\nrnorm(10, mean=0, sd=1)\n\n [1]  0.29238054 -0.62503064 -0.11531518 -0.25313612  0.54876076 -0.02220576\n [7] -1.94152376 -0.53798003  0.35656625  0.31102850",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#exponential-distribution",
    "href": "slides/Chapter2.html#exponential-distribution",
    "title": "Computational Statistics",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\n\nrexp(), dexp(), pexp()\nSimulating exponential data in R",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#properties",
    "href": "slides/Chapter2.html#properties",
    "title": "Computational Statistics",
    "section": "Properties",
    "text": "Properties\n\nMean vector, covariance matrix\nMultivariate normal density\n\n\nlibrary(MASS)\nmvrnorm(n=10, mu=c(0,0), Sigma=matrix(c(1,0.5,0.5,1),2,2))\n\n            [,1]        [,2]\n [1,]  0.2060203  1.46527009\n [2,] -0.9079174  0.11955323\n [3,] -1.3250507 -0.81813971\n [4,]  0.1904166 -1.96895247\n [5,]  0.2525253 -0.45176864\n [6,] -1.3756777 -0.43467478\n [7,] -0.7146808 -0.01001629\n [8,]  1.2684538  0.17021428\n [9,] -0.7458741  0.39571238\n[10,]  0.9693944  1.53632711",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#law-of-large-numbers",
    "href": "slides/Chapter2.html#law-of-large-numbers",
    "title": "Computational Statistics",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\nStatement of the law\nDemonstration using R\n\n\nn &lt;- 1000\nx &lt;- rnorm(n)\ncumsum(x) / 1:n\n\n   [1] -0.13839763  0.35754033  0.60582855  0.82211395  0.86350098  0.67324223\n   [7]  0.92901614  0.70992282  0.62033510  0.55618494  0.29069952  0.33331373\n  [13]  0.49025825  0.34643634  0.26717031  0.26696306  0.20535910  0.25401816\n  [19]  0.28376249  0.32845986  0.34973365  0.34985936  0.31624311  0.30252849\n  [25]  0.25384044  0.21397410  0.19827980  0.16137954  0.14072816  0.17558374\n  [31]  0.18267329  0.21234327  0.19614486  0.17189846  0.17100750  0.22304197\n  [37]  0.22287087  0.20521413  0.21411020  0.20172566  0.22493359  0.22175892\n  [43]  0.19653156  0.22422894  0.17468640  0.14723853  0.14436178  0.12375082\n  [49]  0.14202878  0.11025254  0.09322126  0.10110599  0.08784927  0.09587904\n  [55]  0.10980163  0.08549177  0.09352605  0.10775871  0.11892446  0.14778851\n  [61]  0.18001602  0.17660398  0.17646319  0.15060844  0.16886242  0.15891204\n  [67]  0.13723401  0.13677882  0.12610312  0.13061544  0.13265655  0.14083561\n  [73]  0.13225600  0.14432038  0.14164234  0.11391245  0.13402353  0.15462837\n  [79]  0.16309273  0.16469966  0.13950968  0.14803636  0.15515531  0.15239472\n  [85]  0.15696152  0.16802174  0.18770258  0.17953383  0.17672048  0.15652167\n  [91]  0.15492372  0.15565168  0.15692171  0.16042247  0.14982040  0.14694467\n  [97]  0.12713931  0.12295922  0.12141922  0.11614456  0.11450836  0.10249231\n [103]  0.09757646  0.08808053  0.08837337  0.08741016  0.09359289  0.07456849\n [109]  0.09239706  0.08181836  0.08682111  0.07878505  0.08852951  0.10282952\n [115]  0.09604841  0.09354078  0.08546754  0.07714056  0.07403450  0.07579230\n [121]  0.08412745  0.08204625  0.08065215  0.08622683  0.08605802  0.08195966\n [127]  0.07731926  0.09299368  0.08806580  0.08663601  0.08442919  0.09794095\n [133]  0.10653189  0.11389847  0.11775332  0.11342469  0.09946404  0.10002435\n [139]  0.09513455  0.10806642  0.10749150  0.09588587  0.09978123  0.10443605\n [145]  0.10302608  0.09001832  0.09467616  0.10010210  0.09935481  0.09905434\n [151]  0.09478317  0.09383338  0.10301038  0.10431202  0.10390620  0.10130102\n [157]  0.10506215  0.10117396  0.09843112  0.10257188  0.10261353  0.09835648\n [163]  0.10536916  0.10071089  0.09630732  0.09875199  0.09613554  0.09578949\n [169]  0.08783737  0.08552024  0.09475333  0.10924904  0.11284924  0.10462028\n [175]  0.11771473  0.12436808  0.12233827  0.13124587  0.12685613  0.12810748\n [181]  0.12143451  0.12220403  0.13494184  0.13730656  0.13648473  0.13922731\n [187]  0.13967231  0.13323376  0.13541561  0.13327103  0.12679343  0.12862974\n [193]  0.12613371  0.12374495  0.11830794  0.11334545  0.11429353  0.11490665\n [199]  0.11178076  0.11224036  0.11568044  0.11225718  0.12257653  0.12437995\n [205]  0.11898512  0.12542093  0.12684690  0.12071886  0.11991956  0.11971103\n [211]  0.11806352  0.12922294  0.13195972  0.12943990  0.12938786  0.12995354\n [217]  0.12876607  0.12390699  0.12762854  0.12881529  0.13052766  0.13523334\n [223]  0.13290376  0.13272978  0.12792726  0.11995171  0.11697437  0.11814257\n [229]  0.11523454  0.11237331  0.10590883  0.10481846  0.09896248  0.10217890\n [235]  0.10259771  0.10323631  0.10375688  0.10187576  0.11004138  0.10512869\n [241]  0.10679120  0.09646834  0.08994854  0.08589221  0.09136828  0.09372300\n [247]  0.08717812  0.09467994  0.08913190  0.08920667  0.09179467  0.09549610\n [253]  0.09726457  0.09551037  0.09475664  0.09267444  0.09078878  0.08719262\n [259]  0.08283973  0.07787608  0.07068206  0.07315866  0.07396586  0.07594840\n [265]  0.07482379  0.07209808  0.07184071  0.07269821  0.07558125  0.07744379\n [271]  0.07620538  0.07573867  0.07488729  0.07389495  0.07984073  0.07418156\n [277]  0.07215261  0.07423107  0.07714507  0.07882175  0.08165352  0.08120274\n [283]  0.07367833  0.07357555  0.07865118  0.08214617  0.07502370  0.07732469\n [289]  0.07983516  0.07572714  0.07219381  0.07457725  0.07349938  0.07273468\n [295]  0.07711845  0.07816519  0.08123493  0.07877047  0.08024510  0.08095438\n [301]  0.08051488  0.08054224  0.07684309  0.07350421  0.07255983  0.07372935\n [307]  0.07625113  0.07470354  0.07236742  0.06522616  0.06510047  0.06604130\n [313]  0.06159985  0.06338864  0.06367608  0.06419361  0.06227430  0.06734523\n [319]  0.06637360  0.06615816  0.06722049  0.06332297  0.06039562  0.05720475\n [325]  0.05443979  0.05410747  0.05560550  0.05593293  0.05788989  0.06044809\n [331]  0.05916908  0.05831727  0.05558871  0.05798039  0.05567152  0.05471271\n [337]  0.05164393  0.04947958  0.04755803  0.04417513  0.04636835  0.04733589\n [343]  0.05178312  0.04860464  0.04827528  0.05203667  0.05425134  0.05288086\n [349]  0.05555272  0.05489304  0.05541989  0.05394118  0.05750509  0.05587180\n [355]  0.05360565  0.05068371  0.04618423  0.04646312  0.04800578  0.04662623\n [361]  0.04499075  0.04612712  0.04882746  0.04668011  0.04324118  0.04351456\n [367]  0.04591308  0.04584621  0.04665336  0.04189503  0.04017536  0.03730259\n [373]  0.03477499  0.03656133  0.04404127  0.04310623  0.04108899  0.04279574\n [379]  0.03699253  0.04087735  0.03615278  0.03293172  0.03504395  0.03631198\n [385]  0.03605289  0.04032381  0.04210348  0.04404192  0.04274112  0.04256267\n [391]  0.04355389  0.04143629  0.04419647  0.04489254  0.04566309  0.04548787\n [397]  0.04079097  0.04091394  0.04025712  0.03881433  0.04165626  0.04028378\n [403]  0.04242306  0.03978359  0.03961062  0.03913242  0.03825579  0.03557276\n [409]  0.03322249  0.03465513  0.03012084  0.03050619  0.03018038  0.03324318\n [415]  0.03547748  0.03757347  0.03655204  0.03598711  0.03964432  0.03829609\n [421]  0.03813672  0.03924593  0.04361315  0.04361851  0.04311756  0.04673809\n [427]  0.04723879  0.04650769  0.04494409  0.04080159  0.04183323  0.04250401\n [433]  0.04236225  0.04599270  0.04393430  0.04203736  0.04184745  0.04417874\n [439]  0.04816297  0.04613721  0.04403592  0.04418789  0.04242954  0.04265423\n [445]  0.04351633  0.04643968  0.04513945  0.04065628  0.04253129  0.03903283\n [451]  0.03635902  0.03780173  0.04074780  0.03900940  0.03459660  0.03430021\n [457]  0.03810287  0.03901131  0.03958842  0.03938803  0.04256810  0.03823674\n [463]  0.03823494  0.03630178  0.03866264  0.03732251  0.03886961  0.03865160\n [469]  0.03590759  0.03979435  0.03919888  0.04084326  0.04034004  0.04108040\n [475]  0.03986069  0.03938588  0.03627880  0.03777535  0.03720489  0.03557360\n [481]  0.03367112  0.03490721  0.03513284  0.03597325  0.03852623  0.03539239\n [487]  0.03287721  0.03450992  0.03583571  0.03404014  0.03449680  0.03552821\n [493]  0.03818721  0.03836141  0.03925245  0.03792711  0.03749541  0.03817897\n [499]  0.03968069  0.04306304  0.04104185  0.03817448  0.03822286  0.04081370\n [505]  0.04279146  0.04312743  0.04475094  0.03960530  0.04106670  0.03903147\n [511]  0.03915121  0.03506621  0.03317491  0.03304010  0.03137852  0.03026722\n [517]  0.02711983  0.02729669  0.02959175  0.03109495  0.02986770  0.03168488\n [523]  0.03724304  0.03866117  0.03868803  0.03796459  0.03846832  0.03897455\n [529]  0.03902608  0.04166035  0.04233112  0.04279759  0.04376705  0.04464920\n [535]  0.04472004  0.04243277  0.04133174  0.04384764  0.04767891  0.04711552\n [541]  0.05161465  0.05373753  0.05316704  0.05487528  0.05463400  0.05465182\n [547]  0.05338912  0.05245770  0.05135885  0.05055285  0.04945809  0.04979724\n [553]  0.05121271  0.04999146  0.04720825  0.04638384  0.04927300  0.04709203\n [559]  0.04844277  0.04848671  0.04699572  0.04974367  0.05100148  0.05131978\n [565]  0.05240201  0.05606033  0.06032087  0.06122328  0.06020742  0.05868559\n [571]  0.06267636  0.06432014  0.06504192  0.06515717  0.06579512  0.06606523\n [577]  0.06808365  0.06720626  0.06695507  0.06904124  0.07203879  0.07323648\n [583]  0.07465227  0.07309954  0.07584418  0.07637904  0.07452349  0.07386241\n [589]  0.07610366  0.07730979  0.07510936  0.07635625  0.07609358  0.07481039\n [595]  0.07747050  0.07372487  0.07510543  0.07387236  0.07474328  0.07268764\n [601]  0.07370519  0.07377764  0.07286674  0.07321229  0.07702213  0.07441832\n [607]  0.07540264  0.07415388  0.07390247  0.07221016  0.07273473  0.07643060\n [613]  0.07785076  0.08143152  0.08101633  0.08000507  0.08034499  0.08231462\n [619]  0.08165534  0.08111967  0.08073445  0.07795075  0.07785272  0.07804734\n [625]  0.07756388  0.07686388  0.07711717  0.07666527  0.07348786  0.07314620\n [631]  0.07268678  0.07188068  0.06926546  0.07051136  0.07215008  0.07107851\n [637]  0.07317419  0.07479346  0.07361048  0.07304921  0.07204049  0.07504847\n [643]  0.07300435  0.07333419  0.07482338  0.07346240  0.07305798  0.07275625\n [649]  0.07339204  0.07675559  0.07525270  0.07570905  0.07889970  0.07921311\n [655]  0.07953163  0.08025909  0.07648326  0.07497605  0.07493199  0.07410779\n [661]  0.07416095  0.07242062  0.07138503  0.06927917  0.06736984  0.06702883\n [667]  0.06661576  0.06598093  0.06421455  0.06432389  0.06391590  0.06421724\n [673]  0.06697364  0.06718575  0.06698614  0.06577311  0.06359866  0.06275610\n [679]  0.06316615  0.06745931  0.06597660  0.06519951  0.06776620  0.06894264\n [685]  0.06964892  0.07017779  0.06987682  0.06802909  0.06579949  0.06547446\n [691]  0.06222152  0.06291828  0.06166284  0.06306046  0.06117898  0.06176998\n [697]  0.06518543  0.06318203  0.06237701  0.06268127  0.06406075  0.06285515\n [703]  0.06230689  0.06317433  0.06413917  0.06315057  0.06316136  0.06403346\n [709]  0.06532684  0.06541560  0.06346612  0.06212490  0.06227405  0.06169640\n [715]  0.06256389  0.06397166  0.06605442  0.06832150  0.06847033  0.06864602\n [721]  0.07193616  0.07411985  0.07562617  0.07694984  0.07539424  0.07289159\n [727]  0.07117545  0.07082782  0.07082518  0.07132634  0.07292353  0.07146223\n [733]  0.07372538  0.07380331  0.07278672  0.07121788  0.07089206  0.06893931\n [739]  0.07007806  0.06955201  0.07107675  0.07099622  0.07309357  0.07185813\n [745]  0.07131044  0.07065847  0.06900721  0.06906711  0.06805069  0.06782955\n [751]  0.06903738  0.06852427  0.06841190  0.06813928  0.06743385  0.06600868\n [757]  0.06677829  0.06898021  0.06679918  0.06754824  0.06690812  0.06916007\n [763]  0.06958983  0.07111794  0.07030266  0.06855950  0.06871862  0.06794015\n [769]  0.06584050  0.06404549  0.06204342  0.06241657  0.06404617  0.06417790\n [775]  0.06351684  0.06134301  0.06069648  0.06192519  0.06332352  0.06081652\n [781]  0.05723286  0.05880283  0.06090772  0.06030284  0.06133908  0.06256378\n [787]  0.06174284  0.06162823  0.06191010  0.06206031  0.06014940  0.05899329\n [793]  0.05833602  0.05559942  0.05549684  0.05851341  0.05876811  0.05917909\n [799]  0.05829368  0.05746411  0.05540132  0.05450749  0.05494633  0.05325112\n [805]  0.05207068  0.05062302  0.05121467  0.05144985  0.05284178  0.05354763\n [811]  0.05245016  0.05186467  0.05045021  0.04983667  0.04939394  0.04808142\n [817]  0.04876483  0.04943687  0.04941319  0.04793717  0.05059147  0.05188629\n [823]  0.05119120  0.05072838  0.05040250  0.05093390  0.05097227  0.05267799\n [829]  0.05232011  0.05179623  0.05164077  0.05157847  0.05062052  0.04979318\n [835]  0.04861449  0.04899302  0.04817613  0.04986944  0.05012660  0.05111240\n [841]  0.05044959  0.05079319  0.05082258  0.05031205  0.05052605  0.04900644\n [847]  0.04847127  0.04646595  0.04605828  0.04583336  0.04510872  0.04527246\n [853]  0.04580079  0.04516703  0.04578724  0.04600581  0.04614257  0.04604079\n [859]  0.04592820  0.04513033  0.04677358  0.04844522  0.04899010  0.04860037\n [865]  0.04932174  0.04838091  0.04689314  0.04585330  0.04502589  0.04426116\n [871]  0.04476042  0.04582678  0.04482193  0.04581617  0.04570780  0.04664846\n [877]  0.04717978  0.05085132  0.04948694  0.04994957  0.04946659  0.04990933\n [883]  0.04826724  0.04739902  0.04694423  0.04985450  0.05096800  0.04992252\n [889]  0.04958659  0.04914195  0.05003736  0.04944371  0.04948265  0.04987066\n [895]  0.05068740  0.04981084  0.04950216  0.04933393  0.05019035  0.04975007\n [901]  0.04895632  0.04617597  0.04427722  0.04310966  0.04491412  0.04516208\n [907]  0.04482655  0.04542246  0.04726365  0.04743146  0.04551068  0.04657496\n [913]  0.04538775  0.04454340  0.04286656  0.04363993  0.04355223  0.04367018\n [919]  0.04398383  0.04602731  0.04284034  0.04100169  0.04099002  0.04093094\n [925]  0.04111023  0.04256156  0.04279695  0.04453528  0.04715219  0.04447918\n [931]  0.04340715  0.04177128  0.04057107  0.03984233  0.03875987  0.03982643\n [937]  0.03967508  0.03933680  0.03999162  0.03887562  0.03630019  0.03565735\n [943]  0.03496299  0.03438010  0.03532917  0.03657940  0.03536506  0.03583640\n [949]  0.03707766  0.03755086  0.03759731  0.03855702  0.03899610  0.03731726\n [955]  0.03844036  0.03810507  0.03875879  0.03757924  0.03891467  0.03780197\n [961]  0.03702921  0.03715536  0.03721620  0.03679153  0.03549667  0.03395334\n [967]  0.03329600  0.03211293  0.03116305  0.03193753  0.03196398  0.03111615\n [973]  0.03080523  0.03012628  0.02828619  0.02785236  0.02773759  0.02712329\n [979]  0.02775519  0.02698904  0.02631451  0.02609197  0.02695750  0.02657887\n [985]  0.02651364  0.02631519  0.02714243  0.02490750  0.02397698  0.02676591\n [991]  0.02888114  0.02907225  0.02888718  0.02937448  0.03014539  0.02905625\n [997]  0.02845324  0.02831520  0.02773373  0.02737315",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#central-limit-theorem",
    "href": "slides/Chapter2.html#central-limit-theorem",
    "title": "Computational Statistics",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nDiscuss the CLT and its importance in statistics\nSimulating the CLT with normal distributions\n\n\n# Central Limit Theorem simulation\nmeans &lt;- replicate(1000, mean(rnorm(100, mean = mu, sd = sig)))\nhist(means, main=\"Central Limit Theorem\", col=\"lightgreen\", border=\"black\")",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter5.html#customizing-scatterplot-matrices",
    "href": "slides/Chapter5.html#customizing-scatterplot-matrices",
    "title": "Computational Statistics",
    "section": "Customizing Scatterplot Matrices",
    "text": "Customizing Scatterplot Matrices\n\nCustomizing scatterplot matrices with density plots\n\n\n# Adding density plots to scatterplot matrix\npanel.d &lt;- function(x, ...) {\n    usr &lt;- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(usr[1:2], 0, .5))\n    lines(density(x), col=\"red\")\n}\npairs(iris[101:150, 1:4], panel = panel.d, main=\"Customized Scatterplot Matrix\")",
    "crumbs": [
      "Slides",
      "Visualization"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#numerical-precision-and-floating-point-arithmetic",
    "href": "slides/Chapter13.html#numerical-precision-and-floating-point-arithmetic",
    "title": "Computational Statistics",
    "section": "Numerical Precision and Floating-Point Arithmetic",
    "text": "Numerical Precision and Floating-Point Arithmetic\n\nIntroduction to floating-point arithmetic in R\nBrain teasers: TRUE or FALSE?\n\n\n# Examples of floating-point precision\n1==1\n\n[1] TRUE\n\n3-2==1\n\n[1] TRUE\n\n\n\n\n0.3-0.2==0.1\n\n[1] FALSE\n\n\n\n\n0.4-0.2==0.2\n\n[1] TRUE",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#binary-representation",
    "href": "slides/Chapter13.html#binary-representation",
    "title": "Computational Statistics",
    "section": "Binary Representation",
    "text": "Binary Representation\n\nDiscussion on binary fractions and limitations\nResources:\n\nBinary Fraction Calculator\nFloating Point Problem",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#iterative-methods",
    "href": "slides/Chapter13.html#iterative-methods",
    "title": "Computational Statistics",
    "section": "Iterative Methods",
    "text": "Iterative Methods\n\nIntroduction to iterative methods in numerical analysis\n\n\n# Example: Calculating large integers\nas.integer(2^31-1)\n\n[1] 2147483647\n\nas.integer(2^31)\n\n[1] NA",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#numerical-integration",
    "href": "slides/Chapter13.html#numerical-integration",
    "title": "Computational Statistics",
    "section": "Numerical Integration",
    "text": "Numerical Integration\n\nExample of numerical integration using base R functions\n\n\n# Numerical integration using integrate function\nf &lt;- function(x) sin(x)\nintegrate(f, lower = 0, upper = pi)\n\n2 with absolute error &lt; 2.2e-14",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#conclusion",
    "href": "slides/Chapter13.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of numerical precision, floating-point arithmetic, and numerical integration methods\nPractice: Apply numerical methods to solve optimization and integration problems\n\n\n\n\n\nüîó tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for MATH 4750 (MSSC 5750) - Computational Statistics by Dr.¬†Mehdi Maadooliat in Spring 2025 at Marquette University. All course materials will be posted on this site.\nYou can find the course syllabus here (course flyer here) and the course schedule here.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#class-meetings",
    "href": "course-overview.html#class-meetings",
    "title": "Course overview",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nDavid Straz Nursing 402\nTue & Thur 2:00 - 3:15 pm\n\n\nOffice Hours\nCU 351\nTue & Thur 12:25 - 1:55 pm",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  }
]