[
  {
    "objectID": "slides/Chapter2.html#random-variables-and-distributions",
    "href": "slides/Chapter2.html#random-variables-and-distributions",
    "title": "Computational Statistics",
    "section": "Random Variables and Distributions",
    "text": "Random Variables and Distributions\n\nOverview of random variables\nDiscrete vs Continuous random variables\nImportant distributions (Binomial, Normal, Chi-Square)\n\n\n# Simulating random variables\nN &lt;- 100; mu &lt;- 5; sig &lt;- 2\nY &lt;- rnorm(N, mean = mu, sd = sig)\nhist(Y, main=\"Histogram of Normal Distribution\", xlab=\"Y values\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#binomial-distribution",
    "href": "slides/Chapter2.html#binomial-distribution",
    "title": "Computational Statistics",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nrbinom(), dbinom(), pbinom()\nExample: Generating binomial random variables\n\n\nrbinom(10, size=10, prob=0.5)\n\n [1] 6 6 7 9 9 5 4 4 3 6",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#poisson-distribution",
    "href": "slides/Chapter2.html#poisson-distribution",
    "title": "Computational Statistics",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nrpois(), dpois(), ppois()\nExample: Simulating Poisson-distributed data\n\n\nrpois(10, lambda=2)\n\n [1] 2 3 3 4 2 2 0 4 1 2",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#normal-distribution",
    "href": "slides/Chapter2.html#normal-distribution",
    "title": "Computational Statistics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nrnorm(), dnorm(), pnorm()\nExample: Generating normal random variables\n\n\nrnorm(10, mean=0, sd=1)\n\n [1] -1.64232620  1.16442667  2.13737972 -1.37818889 -0.04291018  0.22422148\n [7] -0.08760599  0.49242418 -1.01950880 -0.01590301",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#exponential-distribution",
    "href": "slides/Chapter2.html#exponential-distribution",
    "title": "Computational Statistics",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\n\nrexp(), dexp(), pexp()\nSimulating exponential data in R",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#properties",
    "href": "slides/Chapter2.html#properties",
    "title": "Computational Statistics",
    "section": "Properties",
    "text": "Properties\n\nMean vector, covariance matrix\nMultivariate normal density\n\n\nlibrary(MASS)\nmvrnorm(n=10, mu=c(0,0), Sigma=matrix(c(1,0.5,0.5,1),2,2))\n\n              [,1]        [,2]\n [1,]  2.340402567 -0.54960140\n [2,] -0.243475428  0.04927353\n [3,]  0.965480695  0.80193569\n [4,]  1.675032703  0.64605060\n [5,]  1.075907390  0.23854826\n [6,]  0.083103190 -0.03031788\n [7,] -0.007367192 -0.30166216\n [8,]  0.712044537  0.56390432\n [9,] -1.010380769  0.21354449\n[10,] -0.270756429  0.61572904",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#law-of-large-numbers",
    "href": "slides/Chapter2.html#law-of-large-numbers",
    "title": "Computational Statistics",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\nStatement of the law\nDemonstration using R\n\n\nn &lt;- 1000\nx &lt;- rnorm(n)\ncumsum(x) / 1:n\n\n   [1] 1.079668239 0.975164051 1.157853930 0.952773898 0.963470551 1.002093566\n   [7] 0.746668797 0.614400823 0.588260149 0.539236297 0.544874528 0.502807073\n  [13] 0.427055376 0.382166820 0.408358898 0.390633984 0.473733180 0.464342509\n  [19] 0.485089340 0.464952140 0.493876266 0.537166165 0.470168751 0.425651851\n  [25] 0.387113829 0.370619820 0.353371372 0.370409772 0.409373111 0.443246857\n  [31] 0.413722171 0.325594161 0.333081448 0.305786443 0.257191132 0.193652817\n  [37] 0.192377089 0.212231774 0.184612835 0.227298050 0.196099529 0.182923556\n  [43] 0.175179232 0.166316975 0.177486468 0.136032321 0.116868497 0.104742306\n  [49] 0.119873189 0.124651839 0.140833500 0.124598532 0.106882965 0.080029521\n  [55] 0.108326488 0.090661598 0.096076434 0.082400882 0.097522733 0.091309840\n  [61] 0.085833879 0.095996011 0.081483063 0.070553117 0.074557306 0.050087369\n  [67] 0.029397834 0.046764996 0.059628401 0.082067586 0.078919720 0.085623423\n  [73] 0.094639147 0.093059000 0.084486211 0.091930880 0.086335450 0.056550722\n  [79] 0.088149813 0.087795067 0.095330854 0.076647405 0.080034921 0.095429934\n  [85] 0.098854853 0.111472251 0.120267944 0.126513252 0.131903494 0.132164494\n  [91] 0.138775811 0.116005920 0.129086987 0.128158467 0.111316991 0.118248186\n  [97] 0.114059752 0.106797981 0.110338750 0.095398316 0.116437816 0.132668236\n [103] 0.139617866 0.133204455 0.127683140 0.132485278 0.118330160 0.108843618\n [109] 0.110271730 0.115642675 0.118592848 0.109803238 0.114638614 0.119000565\n [115] 0.125648714 0.127038530 0.142654265 0.149074912 0.142491917 0.145553518\n [121] 0.152569148 0.152297123 0.135509110 0.137126534 0.125198548 0.121847177\n [127] 0.119177285 0.123639799 0.117695324 0.111938099 0.123925049 0.112877935\n [133] 0.120930456 0.128012489 0.117897927 0.123398455 0.119772886 0.121766212\n [139] 0.117973928 0.126344950 0.117486070 0.125602583 0.132116545 0.132499026\n [145] 0.130582181 0.125560496 0.117955627 0.127311681 0.134338935 0.134406367\n [151] 0.130832395 0.136801228 0.142691883 0.144177757 0.142672361 0.139584840\n [157] 0.138836986 0.136810993 0.131003280 0.129653798 0.129134004 0.128313653\n [163] 0.124046004 0.114525320 0.113156884 0.110095979 0.111883537 0.121079973\n [169] 0.114651877 0.119485664 0.115467547 0.117792289 0.118373731 0.110804275\n [175] 0.110261454 0.105982531 0.110684300 0.098515803 0.098367704 0.103350853\n [181] 0.102454047 0.104789693 0.108246977 0.112139364 0.111696085 0.112284574\n [187] 0.115310009 0.114828885 0.117877177 0.120966500 0.122386648 0.121303812\n [193] 0.117589062 0.114180461 0.116913530 0.112248615 0.118280356 0.116800844\n [199] 0.104282398 0.098527279 0.103542441 0.105282808 0.106672623 0.108344301\n [205] 0.102280741 0.093264334 0.090756976 0.087713361 0.087113044 0.083531358\n [211] 0.093255340 0.099054147 0.101825206 0.112155647 0.110501769 0.114388903\n [217] 0.113272955 0.114246170 0.118054034 0.121164847 0.125005201 0.127749668\n [223] 0.133095122 0.132464375 0.127733484 0.129958976 0.128291302 0.131760867\n [229] 0.132062557 0.132237338 0.135079335 0.129783118 0.129016108 0.131753848\n [235] 0.136820687 0.129449241 0.125479770 0.117297855 0.109213574 0.108771620\n [241] 0.110797717 0.108764480 0.108097392 0.101824422 0.099339339 0.097477749\n [247] 0.098077828 0.095063542 0.098907909 0.095246506 0.095048089 0.089630053\n [253] 0.092967341 0.090190754 0.088322087 0.085980858 0.083198516 0.086223682\n [259] 0.082951376 0.081703598 0.081998126 0.080665148 0.079058930 0.083595970\n [265] 0.084578558 0.082172094 0.074781451 0.071871442 0.068155995 0.068796199\n [271] 0.067456652 0.066635347 0.067805572 0.071432616 0.064399387 0.064132829\n [277] 0.061187843 0.054843068 0.053104638 0.057651536 0.056556278 0.056241768\n [283] 0.056636096 0.054971318 0.051282359 0.054927372 0.056344156 0.057380249\n [289] 0.057926539 0.056153726 0.056039336 0.062307215 0.061122155 0.065997494\n [295] 0.069916739 0.070012862 0.069169714 0.065009895 0.071233303 0.072438625\n [301] 0.079312829 0.080377885 0.077034058 0.078646551 0.081321030 0.076995581\n [307] 0.078747856 0.082182927 0.081464086 0.080452530 0.074078263 0.069627708\n [313] 0.066974712 0.067589914 0.063713214 0.064800886 0.065951933 0.062809449\n [319] 0.067222142 0.068365582 0.064546242 0.065878083 0.056369046 0.053245272\n [325] 0.056504826 0.060420378 0.054529604 0.057587014 0.054128613 0.051565934\n [331] 0.048794294 0.050307208 0.053902591 0.050300763 0.053093946 0.056039754\n [337] 0.054400437 0.055341659 0.054064893 0.047697701 0.044385512 0.042348983\n [343] 0.042346359 0.037955604 0.036343781 0.031681519 0.026879227 0.024648835\n [349] 0.021168012 0.021440151 0.021511542 0.020837090 0.017178480 0.015456048\n [355] 0.014239105 0.012301576 0.018793376 0.019923633 0.023238820 0.025446469\n [361] 0.027965316 0.029589400 0.028589253 0.032335594 0.029078630 0.030506639\n [367] 0.031905065 0.031541217 0.035655931 0.029891822 0.032616581 0.033117231\n [373] 0.032709500 0.034457259 0.031661211 0.029110216 0.030698340 0.027469643\n [379] 0.026520784 0.024423065 0.025727466 0.027945747 0.028084948 0.025031511\n [385] 0.023629466 0.027246399 0.027126523 0.029008520 0.028887341 0.025805730\n [391] 0.025544764 0.021537481 0.019053999 0.019330914 0.018423811 0.018234628\n [397] 0.020283587 0.025413397 0.026950294 0.028790650 0.031629358 0.029229790\n [403] 0.027069531 0.024441105 0.021937277 0.022523188 0.019604149 0.024592516\n [409] 0.022943206 0.024653723 0.025333765 0.028762843 0.028293639 0.028281931\n [415] 0.028467732 0.031990693 0.032872797 0.032038182 0.029641916 0.031350012\n [421] 0.029041519 0.033336778 0.032469530 0.032566829 0.029468226 0.030405050\n [427] 0.026295461 0.025331308 0.027857918 0.024061376 0.024438590 0.019529687\n [433] 0.019137701 0.017380447 0.016811833 0.016227778 0.014067334 0.020536798\n [439] 0.019072775 0.020551674 0.021398070 0.024500971 0.025214778 0.026470807\n [445] 0.026802231 0.022942572 0.026494766 0.029438111 0.029022114 0.029168835\n [451] 0.027604776 0.028641804 0.028001977 0.031564950 0.032126517 0.033214099\n [457] 0.034867168 0.033342343 0.032758086 0.031316648 0.031742079 0.026673832\n [463] 0.023667354 0.024796732 0.021770167 0.020115619 0.023310363 0.025900423\n [469] 0.026223869 0.027891572 0.027113686 0.026049315 0.026024095 0.021689335\n [475] 0.022010987 0.018785198 0.020595154 0.020728974 0.019405965 0.018726742\n [481] 0.020546929 0.021528505 0.021592010 0.028447158 0.029879555 0.030864686\n [487] 0.029546656 0.029423523 0.028942821 0.025611644 0.020699500 0.015291966\n [493] 0.015605788 0.014599008 0.014241900 0.014617908 0.011862828 0.008910904\n [499] 0.006789516 0.005571695 0.006591243 0.007877052 0.007108978 0.006371201\n [505] 0.006747199 0.004400703 0.006381317 0.006611543 0.007957822 0.009191014\n [511] 0.005937660 0.004599166 0.004665847 0.006172888 0.006978489 0.009818465\n [517] 0.011357386 0.015802759 0.017331279 0.019973511 0.020735045 0.024731252\n [523] 0.025685943 0.025827939 0.024020462 0.024698353 0.025013413 0.023339454\n [529] 0.027092951 0.026112150 0.029273346 0.030244585 0.029022888 0.031932627\n [535] 0.028908044 0.029312839 0.030110415 0.031498556 0.032436506 0.030961880\n [541] 0.029234484 0.030015511 0.031665390 0.032244272 0.032153255 0.032443308\n [547] 0.032333671 0.032119282 0.032983280 0.031271335 0.030517903 0.030723113\n [553] 0.033675792 0.032685974 0.032127134 0.028007548 0.029445584 0.031628778\n [559] 0.032510334 0.031093278 0.032984732 0.032522342 0.035659574 0.036462367\n [565] 0.037594158 0.038673662 0.037864366 0.038944461 0.038109394 0.035811921\n [571] 0.036306595 0.035714805 0.035587241 0.037907420 0.037892272 0.035214435\n [577] 0.035072451 0.035303174 0.035711734 0.035405053 0.036165608 0.034369132\n [583] 0.030931693 0.031074178 0.032814497 0.032392808 0.034253778 0.032998419\n [589] 0.034333944 0.033039825 0.034893129 0.032731451 0.030968017 0.028900222\n [595] 0.029248649 0.029773652 0.029480864 0.029303128 0.028059315 0.028716279\n [601] 0.032094046 0.032021291 0.032349667 0.034847938 0.037252514 0.036875604\n [607] 0.038699806 0.039418985 0.038646731 0.037142017 0.034885024 0.034732493\n [613] 0.036794778 0.037554984 0.037340785 0.036625146 0.033653917 0.034644694\n [619] 0.035320599 0.032822210 0.032968429 0.032221913 0.029977760 0.030195854\n [625] 0.030250286 0.031043171 0.030488204 0.030341080 0.031637155 0.032717592\n [631] 0.031767795 0.030634617 0.030638750 0.032213579 0.030087702 0.031750762\n [637] 0.032056213 0.029600236 0.031495738 0.032849709 0.033073437 0.034951877\n [643] 0.036227054 0.035665539 0.036338735 0.039722395 0.039475231 0.038466385\n [649] 0.039675413 0.040970368 0.040949431 0.040701590 0.041769935 0.042243100\n [655] 0.044772212 0.042040719 0.044966223 0.044173054 0.045218922 0.044786816\n [661] 0.043842041 0.043904111 0.043486522 0.043291193 0.044567911 0.045585161\n [667] 0.043863928 0.045928718 0.044506872 0.044465671 0.045414700 0.044285365\n [673] 0.042451956 0.043260789 0.042534254 0.044118426 0.045093896 0.044315776\n [679] 0.042443220 0.040862397 0.039772530 0.038822047 0.038726518 0.039926925\n [685] 0.039441067 0.038259053 0.038005895 0.040121229 0.040138456 0.039498691\n [691] 0.039963749 0.039810645 0.037753186 0.038169976 0.037706004 0.037749107\n [697] 0.037162990 0.035236369 0.034718886 0.034149389 0.035506804 0.035896786\n [703] 0.039430602 0.039473422 0.039435681 0.037098144 0.038637759 0.041387424\n [709] 0.040364636 0.040686375 0.041684606 0.040771417 0.042711624 0.041892103\n [715] 0.042316817 0.041576007 0.042672175 0.043283030 0.042109279 0.042630044\n [721] 0.046200624 0.045213328 0.046906804 0.047077185 0.047379842 0.048081708\n [727] 0.047411075 0.046199453 0.047097947 0.044824677 0.042634757 0.041240418\n [733] 0.041620095 0.038512610 0.036954178 0.034824250 0.035696613 0.035360693\n [739] 0.035689664 0.037370302 0.037408178 0.038980620 0.037610303 0.037885643\n [745] 0.038018864 0.038611528 0.039505658 0.042345638 0.042642377 0.042512689\n [751] 0.043849703 0.045266114 0.046237445 0.046986682 0.046686916 0.046550523\n [757] 0.048799081 0.046517603 0.045546092 0.047854513 0.048882565 0.046987395\n [763] 0.048339059 0.048486930 0.047871920 0.049854470 0.051553178 0.050602812\n [769] 0.049919887 0.049851141 0.046688619 0.045529257 0.045523881 0.044380872\n [775] 0.044372426 0.045218288 0.043556206 0.042987190 0.044332564 0.043152746\n [781] 0.041855083 0.041283094 0.040334143 0.041618658 0.039872960 0.041541845\n [787] 0.039972536 0.039618777 0.039454865 0.042858463 0.043695999 0.043511046\n [793] 0.042269634 0.042667575 0.043718673 0.044956097 0.045089397 0.042654825\n [799] 0.040029752 0.038667417 0.036859701 0.036542135 0.036090449 0.037071021\n [805] 0.037482657 0.036040131 0.037771853 0.039991090 0.040404283 0.039451241\n [811] 0.040211778 0.039087607 0.039795483 0.038439301 0.038874668 0.041845656\n [817] 0.042799259 0.043243590 0.040295596 0.041078638 0.039532139 0.040925373\n [823] 0.042123376 0.041342647 0.042876614 0.041388102 0.038472332 0.038604328\n [829] 0.039545428 0.040641406 0.040058269 0.038798532 0.038078832 0.037516266\n [835] 0.034944866 0.032364877 0.034727114 0.033343812 0.033692983 0.033653993\n [841] 0.034316018 0.033354548 0.032581512 0.034848560 0.034797656 0.033889746\n [847] 0.034362639 0.033854053 0.032955692 0.033257393 0.033785705 0.033710025\n [853] 0.034061033 0.034937215 0.038324282 0.038218606 0.039390513 0.038014789\n [859] 0.035916938 0.036023808 0.035939877 0.034750799 0.036095519 0.034740485\n [865] 0.034561530 0.034231084 0.032308658 0.034568010 0.030964498 0.031804844\n [871] 0.031610749 0.032419938 0.032132138 0.031116014 0.032084324 0.030842478\n [877] 0.029987969 0.030446638 0.030896938 0.030085043 0.029994903 0.029820740\n [883] 0.029557340 0.028622806 0.030150176 0.030088706 0.029478687 0.026669109\n [889] 0.026130783 0.026429594 0.024288747 0.023830915 0.025784642 0.024856036\n [895] 0.022747366 0.023173160 0.022884959 0.021274729 0.020360745 0.022215910\n [901] 0.023299515 0.025854999 0.028197512 0.027435085 0.028890160 0.028062216\n [907] 0.027622288 0.028034602 0.025059023 0.022732551 0.024062380 0.023554341\n [913] 0.021956738 0.023768519 0.024914925 0.022423034 0.022259209 0.021652129\n [919] 0.021529739 0.022081559 0.020293402 0.021158947 0.020570381 0.020259487\n [925] 0.018517831 0.016139663 0.016379063 0.015207098 0.015321473 0.015581009\n [931] 0.013890549 0.012694322 0.012165580 0.012023542 0.012172215 0.013227163\n [937] 0.014370552 0.016327681 0.014589384 0.013290640 0.014152221 0.014341193\n [943] 0.015351704 0.014209434 0.014616712 0.012475585 0.012877823 0.012857523\n [949] 0.013719500 0.013871564 0.013546568 0.013181160 0.011704911 0.012684573\n [955] 0.013513616 0.012993969 0.013363605 0.014071532 0.013883168 0.012894995\n [961] 0.011813773 0.011323212 0.011513157 0.011981833 0.010493259 0.011424022\n [967] 0.010181553 0.011665869 0.010534808 0.010242895 0.009335449 0.008416973\n [973] 0.007074072 0.006298809 0.008194557 0.009299128 0.008620488 0.008552640\n [979] 0.008256707 0.009357962 0.009512372 0.009462681 0.009920764 0.009344377\n [985] 0.007707930 0.006773013 0.006893328 0.006851562 0.006855849 0.005078761\n [991] 0.005410283 0.004439160 0.003103590 0.003994967 0.003240036 0.002016015\n [997] 0.001761014 0.002662740 0.003001523 0.004266433",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter2.html#central-limit-theorem",
    "href": "slides/Chapter2.html#central-limit-theorem",
    "title": "Computational Statistics",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nDiscuss the CLT and its importance in statistics\nSimulating the CLT with normal distributions\n\n\n# Central Limit Theorem simulation\nmeans &lt;- replicate(1000, mean(rnorm(100, mean = mu, sd = sig)))\nhist(means, main=\"Central Limit Theorem\", col=\"lightgreen\", border=\"black\")",
    "crumbs": [
      "Slides",
      "Proba. and Stat. Review"
    ]
  },
  {
    "objectID": "slides/Chapter12.html#histogram-density-estimates",
    "href": "slides/Chapter12.html#histogram-density-estimates",
    "title": "Computational Statistics",
    "section": "Histogram Density Estimates",
    "text": "Histogram Density Estimates\n\nIntroduction to density estimation using histograms\nExample: Histogram density estimates using Sturges’ Rule\n\n\nset.seed(12345)\nn &lt;- 25\nx &lt;- rnorm(n)\n\n# Calculate breaks according to Sturges' Rule\nnclass &lt;- ceiling(1 + log2(n))\ncwidth &lt;- diff(range(x) / nclass)\nbreaks &lt;- min(x) + cwidth * 0:nclass\n\n# Default histogram\nh.default &lt;- hist(x, freq = FALSE, xlab = \"default\", main = \"Histogram: Default\")",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter12.html#kernel-density-estimation",
    "href": "slides/Chapter12.html#kernel-density-estimation",
    "title": "Computational Statistics",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\n\nIntroduction to kernel density estimation (KDE)\nExample: Applying KDE to data\n\n\n# Kernel density estimate\ndens &lt;- density(x)\nplot(dens, main = \"Kernel Density Estimate\", col = \"blue\", lwd = 2)",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter12.html#visualizing-kernel-density",
    "href": "slides/Chapter12.html#visualizing-kernel-density",
    "title": "Computational Statistics",
    "section": "Visualizing Kernel Density",
    "text": "Visualizing Kernel Density\n\n# Overlay histogram and KDE\nhist(x, freq = FALSE, col = \"lightgray\", border = \"white\", main = \"Histogram with KDE\")\nlines(dens, col = \"red\", lwd = 2)",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter12.html#conclusion",
    "href": "slides/Chapter12.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of histogram density estimation and kernel density estimation\nPractice: Apply density estimation techniques to other datasets\n\n\n\n\n\n🔗 tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Density Estimation"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#sampling-from-a-finite-population",
    "href": "slides/Chapter3.html#sampling-from-a-finite-population",
    "title": "Computational Statistics",
    "section": "Sampling from a Finite Population",
    "text": "Sampling from a Finite Population\n\nIntroduction to finite population sampling\nExample: Tossing coins, choosing lottery numbers\n\n\n# Sampling examples\nsample(0:1, size = 10, replace = TRUE)  # Tossing coins\n\n [1] 0 0 0 1 1 0 1 0 0 1\n\nsample(1:100, size = 6, replace = FALSE)  # Choosing lottery numbers\n\n[1] 80 52 83 74 93 13\n\nsample(letters)  # Permutation of letters\n\n [1] \"c\" \"d\" \"r\" \"a\" \"l\" \"x\" \"k\" \"n\" \"z\" \"v\" \"h\" \"f\" \"g\" \"m\" \"j\" \"b\" \"t\" \"p\" \"i\"\n[20] \"u\" \"e\" \"o\" \"s\" \"q\" \"y\" \"w\"",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#multinomial-distribution",
    "href": "slides/Chapter3.html#multinomial-distribution",
    "title": "Computational Statistics",
    "section": "Multinomial Distribution",
    "text": "Multinomial Distribution\n\nIntroduction to multinomial distribution\nExample: Sampling from a multinomial distribution\n\n\n# Sample from multinomial distribution\nx &lt;- sample(1:3, size = 100, replace = TRUE, prob = c(.2, .3, .5))\ntable(x)\n\nx\n 1  2  3 \n25 29 46",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#continuous-case",
    "href": "slides/Chapter3.html#continuous-case",
    "title": "Computational Statistics",
    "section": "Continuous Case",
    "text": "Continuous Case\n\nExplanation of the inverse transform method for continuous distributions\nExample: Simulating exponential random variables\n\n\n# Inverse transform method for exponential distribution\nn &lt;- 1000\nU &lt;- runif(n)\nX &lt;- -log(1 - U)\nhist(X, main=\"Exponential Distribution via Inverse Transform\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#discrete-case",
    "href": "slides/Chapter3.html#discrete-case",
    "title": "Computational Statistics",
    "section": "Discrete Case",
    "text": "Discrete Case\n\nExample of applying the inverse transform method to discrete random variables\n\n\n# Inverse transform method for geometric distribution\np &lt;- 0.5\nX_geom &lt;- ceiling(log(1 - runif(n)) / log(1 - p))\nhist(X_geom, main=\"Geometric Distribution via Inverse Transform\", col=\"lightgreen\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#introduction-to-acceptance-rejection-method",
    "href": "slides/Chapter3.html#introduction-to-acceptance-rejection-method",
    "title": "Computational Statistics",
    "section": "Introduction to Acceptance-Rejection Method",
    "text": "Introduction to Acceptance-Rejection Method\n\nExplanation of the acceptance-rejection method\nExample: Sampling from a target distribution\n\n\n# Acceptance-rejection method example\ntarget &lt;- function(x) { ifelse(x &gt; 0, exp(-x), 0) }\nproposal &lt;- function(x) { dnorm(x, mean = 2, sd = 1) }\n\nX &lt;- rnorm(1000, mean = 2, sd = 1)\naccept &lt;- runif(1000) &lt; target(X) / (1.5 * proposal(X))\n\nhist(X[accept], main=\"Accepted Samples from Target Distribution\", col=\"lightcoral\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#visualizing-results",
    "href": "slides/Chapter3.html#visualizing-results",
    "title": "Computational Statistics",
    "section": "Visualizing Results",
    "text": "Visualizing Results\n\nCompare the target and proposal distributions 31b8e172-b470-440e-83d8-e6b185028602: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:31b8e172-b470-440e-83d8-e6b185028602",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#transformation-of-random-variables",
    "href": "slides/Chapter3.html#transformation-of-random-variables",
    "title": "Computational Statistics",
    "section": "Transformation of Random Variables",
    "text": "Transformation of Random Variables\n\nIntroduction to transformation methods\nExample: Box-Muller transform for generating normal random variables\n\n\n# Box-Muller transform\nn &lt;- 1000\nU1 &lt;- runif(n)\nU2 &lt;- runif(n)\nZ1 &lt;- sqrt(-2 * log(U1)) * cos(2 * pi * U2)\nZ2 &lt;- sqrt(-2 * log(U1)) * sin(2 * pi * U2)\nhist(Z1, main=\"Normal Distribution via Box-Muller\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter3.html#sums-and-mixtures",
    "href": "slides/Chapter3.html#sums-and-mixtures",
    "title": "Computational Statistics",
    "section": "Sums and Mixtures",
    "text": "Sums and Mixtures\n\nExplanation of sums and mixtures of random variables\nExample: Mixture of normals\n\n\n# Mixture of normals\nlibrary(MASS)\nmu1 &lt;- 0; mu2 &lt;- 3; sigma1 &lt;- 1; sigma2 &lt;- 2\np &lt;- 0.3\nX &lt;- ifelse(runif(n) &lt; p, rnorm(n, mu1, sigma1), rnorm(n, mu2, sigma2))\nhist(X, main=\"Mixture of Normal Distributions\", col=\"lightgreen\")",
    "crumbs": [
      "Slides",
      "Generating RVs"
    ]
  },
  {
    "objectID": "slides/Chapter1.html#probability-distributions",
    "href": "slides/Chapter1.html#probability-distributions",
    "title": "Computational Statistics",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\ndnorm(), pnorm(), qnorm()\nExploring statistical tests in R",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#statistical-tests",
    "href": "slides/Chapter1.html#statistical-tests",
    "title": "Computational Statistics",
    "section": "Statistical Tests",
    "text": "Statistical Tests\n\nt.test(), chisq.test()",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#defining-functions",
    "href": "slides/Chapter1.html#defining-functions",
    "title": "Computational Statistics",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nfunction(arglist) expr\nReturn values and default arguments\n\n\nsumdice &lt;- function(n) {\n  k &lt;- sample(1:6, size=n, replace=TRUE)\n  return(sum(k))\n}\nsumdice(2)\n\n[1] 7",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#data-structures-in-r",
    "href": "slides/Chapter1.html#data-structures-in-r",
    "title": "Computational Statistics",
    "section": "Data Structures in R",
    "text": "Data Structures in R\n\nArrays, matrices, and data frames\n\n\n# Creating vectors and matrices\nx &lt;- c(1, 2, 3, 4)\nmatrix_x &lt;- matrix(x, nrow=2, ncol=2)\nprint(matrix_x)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\nExample: Iris data set\n\n\ndata(iris)\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#basic-plots",
    "href": "slides/Chapter1.html#basic-plots",
    "title": "Computational Statistics",
    "section": "Basic Plots",
    "text": "Basic Plots\n\nplot(), hist(), boxplot()\n\n\nCodePlot\n\n\n\nplot(iris);\n# boxplot(iris);\n# hist(iris[,1])",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#introduction-to-ggplot2",
    "href": "slides/Chapter1.html#introduction-to-ggplot2",
    "title": "Computational Statistics",
    "section": "Introduction to ggplot2",
    "text": "Introduction to ggplot2\n\nVisualizing using ggplot2\n\n\nCodePlot\n\n\n\nlibrary(ggplot2)\nggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE, color=\"blue\") + \n  theme_minimal() + \n  labs(title=\"Sepal vs Petal Length in Iris Dataset\", x=\"Sepal Length\", y=\"Petal Length\")",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#managing-files",
    "href": "slides/Chapter1.html#managing-files",
    "title": "Computational Statistics",
    "section": "Managing Files",
    "text": "Managing Files\n\nWorking directories and file input/output\nUsing scripts and automation",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter1.html#dynamic-documents",
    "href": "slides/Chapter1.html#dynamic-documents",
    "title": "Computational Statistics",
    "section": "Dynamic Documents",
    "text": "Dynamic Documents\n\nCreating reports with R Markdown\nIntroduction to knitr package",
    "crumbs": [
      "Slides",
      "Intro. to Stat. Comp."
    ]
  },
  {
    "objectID": "slides/Chapter13.html#numerical-precision-and-floating-point-arithmetic",
    "href": "slides/Chapter13.html#numerical-precision-and-floating-point-arithmetic",
    "title": "Computational Statistics",
    "section": "Numerical Precision and Floating-Point Arithmetic",
    "text": "Numerical Precision and Floating-Point Arithmetic\n\nIntroduction to floating-point arithmetic in R\nBrain teasers: TRUE or FALSE?\n\n\n# Examples of floating-point precision\n1==1\n\n[1] TRUE\n\n3-2==1\n\n[1] TRUE\n\n0.3-0.2==0.1\n\n[1] FALSE\n\n0.4-0.2==0.2\n\n[1] TRUE",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#binary-representation",
    "href": "slides/Chapter13.html#binary-representation",
    "title": "Computational Statistics",
    "section": "Binary Representation",
    "text": "Binary Representation\n\nDiscussion on binary fractions and limitations\nResources:\n\nBinary Fraction Calculator\nFloating Point Problem",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#iterative-methods",
    "href": "slides/Chapter13.html#iterative-methods",
    "title": "Computational Statistics",
    "section": "Iterative Methods",
    "text": "Iterative Methods\n\nIntroduction to iterative methods in numerical analysis\n\n\n# Example: Calculating large integers\nas.integer(2^31-1)\n\n[1] 2147483647\n\nas.integer(2^31)\n\n[1] NA",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#numerical-integration",
    "href": "slides/Chapter13.html#numerical-integration",
    "title": "Computational Statistics",
    "section": "Numerical Integration",
    "text": "Numerical Integration\n\nExample of numerical integration using base R functions\n\n\n# Numerical integration using integrate function\nf &lt;- function(x) sin(x)\nintegrate(f, lower = 0, upper = pi)\n\n2 with absolute error &lt; 2.2e-14",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter13.html#conclusion",
    "href": "slides/Chapter13.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of numerical precision, floating-point arithmetic, and numerical integration methods\nPractice: Apply numerical methods to solve optimization and integration problems\n\n\n\n\n\n🔗 tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Numerical Methods in R"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#basic-monte-carlo-estimation",
    "href": "slides/Chapter7.html#basic-monte-carlo-estimation",
    "title": "Computational Statistics",
    "section": "Basic Monte Carlo Estimation",
    "text": "Basic Monte Carlo Estimation\n\nIntroduction to Monte Carlo estimation\nExample: Estimating the expected difference of two normal variables\n\n\nm &lt;- 1000\ng &lt;- numeric(m)\nfor (i in 1:m) {\n    x &lt;- rnorm(2)\n    g[i] &lt;- abs(x[1] - x[2])\n}\nest &lt;- mean(g)\nest\n\n[1] 1.105716",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#visualizing-monte-carlo-simulations",
    "href": "slides/Chapter7.html#visualizing-monte-carlo-simulations",
    "title": "Computational Statistics",
    "section": "Visualizing Monte Carlo Simulations",
    "text": "Visualizing Monte Carlo Simulations\n\n# Histogram of the simulated differences\nhist(g, main=\"Histogram of Differences (Monte Carlo)\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#estimating-the-mean-squared-error-mse",
    "href": "slides/Chapter7.html#estimating-the-mean-squared-error-mse",
    "title": "Computational Statistics",
    "section": "Estimating the Mean Squared Error (MSE)",
    "text": "Estimating the Mean Squared Error (MSE)\n\nMonte Carlo estimation of MSE for trimmed means\n\n\nn &lt;- 20\nm &lt;- 1000\nmean_trim &lt;- numeric(m)\nfor (i in 1:m) {\n    x &lt;- rnorm(n)\n    mean_trim[i] &lt;- mean(x, trim = 0.1)\n}\nmse &lt;- mean((mean_trim - 0)^2)\nmse\n\n[1] 0.05242377",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#visualizing-mse-simulations",
    "href": "slides/Chapter7.html#visualizing-mse-simulations",
    "title": "Computational Statistics",
    "section": "Visualizing MSE Simulations",
    "text": "Visualizing MSE Simulations\n\n# Histogram of the trimmed means\nhist(mean_trim, main=\"Trimmed Means (Monte Carlo MSE)\", col=\"lightgreen\")",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#additional-monte-carlo-techniques",
    "href": "slides/Chapter7.html#additional-monte-carlo-techniques",
    "title": "Computational Statistics",
    "section": "Additional Monte Carlo Techniques",
    "text": "Additional Monte Carlo Techniques\n\nExploring advanced Monte Carlo techniques for inference\nExample: Hypothesis testing with Monte Carlo methods\n\n\n# Monte Carlo test example: Hypothesis testing\nn &lt;- 50\nm &lt;- 1000\ntest_stat &lt;- numeric(m)\nfor (i in 1:m) {\n    x &lt;- rnorm(n)\n    test_stat[i] &lt;- mean(x)\n}\np_value &lt;- mean(test_stat &gt;= 1.96)\np_value\n\n[1] 0",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "slides/Chapter7.html#conclusion",
    "href": "slides/Chapter7.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of Monte Carlo methods in inference, MSE estimation, and hypothesis testing\nPractice: Apply these methods to other inferential problems\n\n\n\n\n\n🔗 tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Monte Carlo Methods in Inference"
    ]
  },
  {
    "objectID": "hw/HW 2.html",
    "href": "hw/HW 2.html",
    "title": "Computational Statistics - HW 2",
    "section": "",
    "text": "Questions 3.2, 3.3, 3.7, 3.8, 3.11, 3.12, 3.13, and 3.14\n\n6 out of 8 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 2.html#from-textbook",
    "href": "hw/HW 2.html#from-textbook",
    "title": "Computational Statistics - HW 2",
    "section": "",
    "text": "Questions 3.2, 3.3, 3.7, 3.8, 3.11, 3.12, 3.13, and 3.14\n\n6 out of 8 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 4.html",
    "href": "hw/HW 4.html",
    "title": "Computational Statistics - HW 4",
    "section": "",
    "text": "Questions 6.1, 6.2, 6.3, 6.6, 6.7, 6.9, and 6.12 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 4.html#from-textbook",
    "href": "hw/HW 4.html#from-textbook",
    "title": "Computational Statistics - HW 4",
    "section": "",
    "text": "Questions 6.1, 6.2, 6.3, 6.6, 6.7, 6.9, and 6.12 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 3.html",
    "href": "hw/HW 3.html",
    "title": "Computational Statistics - HW 3",
    "section": "",
    "text": "Questions 5.1, 5.2, 5.3, 5.4, 5.5, 5.6 from the Textbook.\n\n5 out of 6 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 3.html#from-textbook",
    "href": "hw/HW 3.html#from-textbook",
    "title": "Computational Statistics - HW 3",
    "section": "",
    "text": "Questions 5.1, 5.2, 5.3, 5.4, 5.5, 5.6 from the Textbook.\n\n5 out of 6 Questions (for Undergraduate students)\nAll Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 7.html",
    "href": "hw/HW 7.html",
    "title": "Computational Statistics - HW 7",
    "section": "",
    "text": "Questions 8.2, 8.10, 8.11, 9.1, 9.4 from the Textbook, and “Course Evaluation”.\n\n4 out of 6 Questions (for Undergraduate students)\n5 out of 6 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 7.html#from-textbook",
    "href": "hw/HW 7.html#from-textbook",
    "title": "Computational Statistics - HW 7",
    "section": "",
    "text": "Questions 8.2, 8.10, 8.11, 9.1, 9.4 from the Textbook, and “Course Evaluation”.\n\n4 out of 6 Questions (for Undergraduate students)\n5 out of 6 Questions (for Graduate students)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nTOPIC\nMATERIALS\nCODE\nDUE\n\n\n\n\n1\nTue, Jan 14\nIntroduction to Statistical Computing\n📚Chapt 1-A\nR👩‍💻\n\n\n\n\n\n\nThu, Jan 16\nProbability and Statistics Review\n📚Chapt 2-A\nR👩‍💻\n\n\n\n\n2\nTue, Jan 21\nMethod of Estimations\n📚💻Chapt 2-B\n\n\n\n\n\n\n\n\nThu, Jan 23\nRegularization in Regression\n💻Ridge and Lasso\n\n\n\n\n\n\n3\nTue, Jan 28\nFunctional Data Analysis and Smoothing\n💻FDA-A\nR👩‍💻\n\n\n\n\n\n\nThu, Jan 30\nMore on FDA and Basics of PCA\n💻FDA-B\n\n\n\n\n\n\n\n\nFri, Jan 31\n\n\n\n\n\n\n📝 HW 1 at 11:50 pm\n\n\n4\nTue, Feb 4\nMethods for Generating Random Variables\n📚Chapt 3-A\nR👩‍💻\n\n\n\n\n\n\nThu, Feb 6\nMore on Generating RV’s\n📚Chapt 3-B\n\n\n\n\n\n\n5\nTue, Feb 11\nK-means Clustering\n💻Clustering-A\nR👩‍💻\n\n\n\n\n\n\nThu, Feb 13\nHirarchical Clustering\n💻Clustering-B\n\n\n\n\n\n\n\n\nFri, Feb 14\n\n\n\n\n\n\n📝 [HW 2] at 11:50 pm\n\n\n6\nTue, Feb 18\nVizualization\n📚Chapt 5\nR👩‍💻\n\n\n\n\n\n\nThu, Feb 20\nMonte Carlo Integration\n📚Chapt 6-A\nR👩‍💻\n\n\n\n\n7\nTue, Feb 25\nImportance Sampling and Variance Reduction\n📚Chapt 6-B\n\n\n\n\n\n\n\n\nThu, Feb 27\nObject Oriented Programming in R\n📖OOP\nR👩‍💻\n\n\n\n\n\n\nFri, Feb 28\n\n\n\n\n\n\n📝 [HW 3] at 11:50 pm\n\n\n8\nTue, Mar 4\nDeveloping an Interactive Shiny App\n📖Shiny\nR👩‍💻\n\n\n\n\n\n\nThu, Mar 6\nDeveloping an R package\n📖R package\n\n\n\n\n\n\n\n\nFri, Mar 7\n\n\n\n\n\n\n📝 [HW 4] at 11:50 pm\n\n\n\n\nTue, Mar 11\n🌴 Spring Break\n\n\n\n\n\n\n\n\n\n\nThu, Mar 13\n🌴 Spring Break\n\n\n\n\n\n\n\n\n9\nTue, Mar 18\nMonte Carlo Integration\n📚Chapt 7-A\nR👩‍💻\n\n\n\n\n\n\nThu, Mar 20\nMore on Monte Carlo Methods\n📚Chapt 7-B  📚💻Chapt 7-C\n\n\n\n\n\n\n10\nTue, Mar 25\nBootstrap\n📚Chapt 8-A\nR👩‍💻\n\n\n\n\n\n\nThu, Mar 27\nMore on Bootstran and Jackknife\n📑Chapt 8-B  🖥️Chapt 8-C  📚Chapt 9\nR👩‍💻\n\n\n\n\n\n\nFri, Mar 28\n\n\n\n\n\n\n📝 [HW 5] at 11:50 pm\n\n\n11\nTue, Apr 1\nDensity Estimation\n📚Chapt 12\nR👩‍💻\n\n\n\n\n\n\nThu, Apr 3\nNumerical Methods in R\n📚Chapt 13\nR👩‍💻\n\n\n\n\n\n\nFri, Apr 4\n\n\n\n\n\n\n📝 [HW 6] at 11:50 pm\n\n\n12\nTue, Apr 8\nOptimization in R\n📚Chapt 14-A\nR👩‍💻\n\n\n\n\n\n\nThu, Apr 10\nMore on Optimization\n📚Chapt 14-B\n\n\n\n\n\n\n\n\nFri, Apr 11\n\n\n\n\n\n\n📝 [HW 7] at 11:50 pm\n\n\n13\nTue, Apr 15\nExpectation-Maximization (EM)-Algorithm\n📑Chapt 14-C\nR👩‍💻\n\n\n\n\n\n\nThu, Apr 17\n🌴 Easter Break\n\n\n\n\n\n\n\n\n14\nTue, Apr 22\nProgramming Topics\n📚Chapt 15\nR👩‍💻\n\n\n\n\n\n\nThu, Apr 24\nDeep Learning in R\n💻DLiR-A\nR👩‍💻\n\n\n\n\n15\nTue, Apr 29\nFully Connected Networks (FCN)\n💻DLiR-B\n\n\n\n\n\n\n\n\nThu, May 1\nConvolutional Nueral Networks (CNN)\n💻DLiR-C\n\n\n\n\n\n\n16\nMon, May 5\n\n\n📃Final Project Presentations : 10:30 am - 12:30 pm",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for MATH 4750 (MSSC 5750) - Computational Statistics by Dr. Mehdi Maadooliat in Spring 2025 at Marquette University. All course materials will be posted on this site.\nYou can find the course syllabus here (course flyer here)and the course schedule here.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#class-meetings",
    "href": "course-overview.html#class-meetings",
    "title": "Course overview",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nCU 351\nTue & Thur 2:00 - 3:15 pm\n\n\nOffice Hours\nCU 351\nTue & Thur 12:25 - 1:55 pm",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus - Computational Statistics",
    "section": "",
    "text": "Course Title: MATH 4750/MSSC 5750: Computational Statistics\nMeeting Time: TuTh 2:00pm - 3:15pm\nLocation: Cudahy Hall 145 (Microsoft Teams)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus - Computational Statistics",
    "section": "",
    "text": "Course Title: MATH 4750/MSSC 5750: Computational Statistics\nMeeting Time: TuTh 2:00pm - 3:15pm\nLocation: Cudahy Hall 145 (Microsoft Teams)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#instructor-details",
    "href": "course-syllabus.html#instructor-details",
    "title": "Syllabus - Computational Statistics",
    "section": "Instructor Details",
    "text": "Instructor Details\n\nName: Mehdi Maadooliat, Ph.D.\nOffice: CU 351\nOffice Hours: TuTh 12:25 - 1:55pm in CU 351 or by e-mail",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus - Computational Statistics",
    "section": "Course Description",
    "text": "Course Description\nComputational statistics and statistical computing are two areas that employ computational, graphical, and numerical approaches to solve statistical problems. Competent statisticians must not just run existing programs but understand the principles behind them. This class introduces statistically-oriented programming targeted at statistics majors, with no extensive programming background assumed.\nThis course covers traditional core material of computational statistics with an emphasis on using R through an example-based approach.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-outcomes",
    "href": "course-syllabus.html#learning-outcomes",
    "title": "Syllabus - Computational Statistics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the course, students will: - Understand core programming concepts, including data structures, functions, iteration, debugging, logical design, and abstraction. - Write maintainable code, debug, and test code for correctness. - Set up and run stochastic simulations in parallel. - Fit basic statistical models and assess the results. - Work with and filter large datasets. - Comment and organize code effectively.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus - Computational Statistics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCOSC 1010\nMATH 3100\nMATH 4700 or MATH 4780",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus - Computational Statistics",
    "section": "Textbooks",
    "text": "Textbooks\n\nMain: Statistical Computing with R, 2nd edition by Maria L. Rizzo. Chapman and Hall/CRC, 2019. ISBN: 1466553324.\nSupplementary: Computational Statistics, 2nd Edition by Geof H. Givens and Jennifer A. Hoeting. Wiley, 2012. ISBN: 9780470533314.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-breakdown",
    "href": "course-syllabus.html#grading-breakdown",
    "title": "Syllabus - Computational Statistics",
    "section": "Grading Breakdown",
    "text": "Grading Breakdown\n\nClass Participation: 10%\nHomework: 30%\nTeam Project: 30%\nFinal Exam or HW/Project (Instructor will decide): 30%\n\n\nGrading Scale\n\n\n\nGrade\nRange\n\n\n\n\nA\n95 - 100%\n\n\nA-\n89 - 94.99%\n\n\nB+\n84 - 88.99%\n\n\nB\n78 - 83.99%\n\n\nB-\n73 - 77.99%\n\n\nC+\n67 - 72.99%\n\n\nC\n62 - 66.99%\n\n\nC-\n56 - 61.99%\n\n\nD+\n51 - 55.99%\n\n\nD\n45 - 50.99%\n\n\nF\n&lt; 44.99%\n\n\n\nNotes:\n\nFor students in MSSC 5750, there will be extra questions in Homework.\nNo late homework will be accepted, and no make-up work will be allowed unless in the case of an emergency. Submit incomplete work if necessary, but ensure scanned PDFs are legible before submission.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#make-up-policy",
    "href": "course-syllabus.html#make-up-policy",
    "title": "Syllabus - Computational Statistics",
    "section": "Make-up Policy",
    "text": "Make-up Policy\nNo make-up exams or homework unless there is an emergency.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#attendance",
    "href": "course-syllabus.html#attendance",
    "title": "Syllabus - Computational Statistics",
    "section": "Attendance",
    "text": "Attendance\nAttendance is required and subject to the College of Arts and Sciences policy.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-honesty",
    "href": "course-syllabus.html#academic-honesty",
    "title": "Syllabus - Computational Statistics",
    "section": "Academic Honesty",
    "text": "Academic Honesty\nStudents are expected to follow the University’s policy on academic honesty as outlined in the Bulletin.\nTL;DR: Don’t cheat!\nPlease abide by the following as you work on assignments in this course:\n\nCollaboration: Only work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must also be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to lab questions (including any code) with anyone other than myself and the teaching assistants.\nThe reading quizzes must be completed individually with absolutely no communication with classmates.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nOn individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\nOnline resources: I am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g., StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of generative artificial intelligence (AI): You should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you.\n\nYou are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask the instructor.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-topics",
    "href": "course-syllabus.html#course-topics",
    "title": "Syllabus - Computational Statistics",
    "section": "Course Topics",
    "text": "Course Topics\n\nManaging Input/Output in R\nData structures in R\nData transformations: strings, factors, dates\nStatistical summaries and basic tests in R\nGraphics in R\nMonte Carlo Integration & Variance Reduction\nBootstrap and Jackknife\nDensity Estimation\nRegularization:\n\nRidge and Lasso\nSmoothing\n\nDimension Reduction and Clustering\n\nK-Means and Hierarchical Clustering\nPrincipal Component Analysis (PCA)\n\nNumerical Methods in R\n\nOptimization\nExpectation-Maximization (EM) Algorithm\n\nAdvanced R Programming\n\nIntroduction to Parallel Computing\nObject Oriented Programming in R\nDeveloping an R Package\nDeveloping an Interactive Shiny App\nFunctions and loops in C and using C in R\n\nDeep Learning in R\n\nBasics of Neural Network\n\nImplementation using TensorFlow and Keras\n\nFully Connected Network (FCN)\nConvolutional Neural Networks (CNN)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus - Computational Statistics",
    "section": "Important dates",
    "text": "Important dates\n\nMonday, January 13: Classes begin\nTuesday, January 21: Drop/add ends\nSaturday - Sunday, March 8 - 16: Spring Break\nFriday, April 11: Last day to withdraw with W\nSaturday, May 3: Classes end\nMonday, May 5, 10:30 am - 12:30 pm: Presentations\n\nFor more important dates, see the full MU Academic Calendar.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-note",
    "href": "course-syllabus.html#important-note",
    "title": "Syllabus - Computational Statistics",
    "section": "Important Note",
    "text": "Important Note\nThe syllabus may be modified throughout the course. Any substantial modifications will result in a reissued syllabus.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Homework\n🔗 on D2L\n\n\nTexbooks\n🔗 Computational Statistics in Python\n🔗 R Package\n🔗 Advanced R by Hadley Wickham\n🔗 Quarto\n🔗 R Markdown\n🔗 ggplot2: Elegant Graphics for Data Analysis\n🔗 Fundamentals of Data Visualization\n🔗 Data Visualization: A Practical Introduction\n🔗 R for Data Science\n\n\nSome Package documentation\n🔗 ggplot2: ggplot2.tidyverse.org\n🔗 dplyr: dplyr.tidyverse.org\n🔗 tidyr: tidyr.tidyverse.org\n🔗 forcats: forcats.tidyverse.org\n🔗 stringr: stringr.tidyverse.org\n🔗 lubridate: lubridate.tidyverse.org\n🔗 readr: readr.tidyverse.org",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Comp-Stat",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Mine Çetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University and an affiliated faculty in the Computational Media, Arts, and Cultures. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTuesdays 2:30 - 3:30 pm\nOld Chem 213 (or Zoom upon request)"
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Mine Çetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University and an affiliated faculty in the Computational Media, Arts, and Cultures. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTuesdays 2:30 - 3:30 pm\nOld Chem 213 (or Zoom upon request)"
  },
  {
    "objectID": "course-team.html#teaching-assistants",
    "href": "course-team.html#teaching-assistants",
    "title": "Teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\nName\nRole\nLab section\nOffice hours\n\n\n\n\n\nHolly Cui\nLab TA\nSection 1 - W 1:25PM - 2:40PM\nTuesdays 11:30 am-12:30 pm in Old Chem 203B\nThursdays 11:30 am-12:30 pm in Old Chem 203B\n\n\n\nEli Gnesin\nHead + Lab TA\nSection 2 - W 3:05PM - 4:20PM\nMonday 10:30-11:30 am in Old Chem 203B\nTuesdays 8:15-9:45 am in Old Chem 203B\n\n\n\nKelsey Brod\nProject lead + Lecture TA\n\nMondays 3-5 pm on Zoom\n\n\n\nShelby Tisdale\nProject support + Lecture TA\n\nSundays 7-8 pm on Zoom\nMondays 9-10 am in Old Chem 025\n\n\n\nGlenn Palmer\n\n\nFridays 2-3 pm in Old Chem 203B"
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "hw/HW 5.html",
    "href": "hw/HW 5.html",
    "title": "Computational Statistics - HW 5",
    "section": "",
    "text": "Questions 7.1, 7.2, 7.3, 7.5, 7.6, 7.7, and 7.8 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 5.html#from-textbook",
    "href": "hw/HW 5.html#from-textbook",
    "title": "Computational Statistics - HW 5",
    "section": "",
    "text": "Questions 7.1, 7.2, 7.3, 7.5, 7.6, 7.7, and 7.8 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 6.html",
    "href": "hw/HW 6.html",
    "title": "Computational Statistics - HW 6",
    "section": "",
    "text": "Questions 8.1, 8.4, 8.5, 8.6, 8.7, 8.8 and 8.9 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 6.html#from-textbook",
    "href": "hw/HW 6.html#from-textbook",
    "title": "Computational Statistics - HW 6",
    "section": "",
    "text": "Questions 8.1, 8.4, 8.5, 8.6, 8.7, 8.8 and 8.9 from the Textbook.\n\n5 out of 7 Questions (for Undergraduate students)\n6 out of 7 Questions (for Graduate students)"
  },
  {
    "objectID": "hw/HW 1.html",
    "href": "hw/HW 1.html",
    "title": "Computational Statistics - HW 1",
    "section": "",
    "text": "Questions 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, and 1.8\n\n7 out of 8 Question (for Undergraduates)\nAll Questions (for Graduates)",
    "crumbs": [
      "HW",
      "HW 1"
    ]
  },
  {
    "objectID": "hw/HW 1.html#from-the-textbook",
    "href": "hw/HW 1.html#from-the-textbook",
    "title": "Computational Statistics - HW 1",
    "section": "",
    "text": "Questions 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, and 1.8\n\n7 out of 8 Question (for Undergraduates)\nAll Questions (for Graduates)",
    "crumbs": [
      "HW",
      "HW 1"
    ]
  },
  {
    "objectID": "hw/HW 1.html#extra-question",
    "href": "hw/HW 1.html#extra-question",
    "title": "Computational Statistics - HW 1",
    "section": "Extra Question",
    "text": "Extra Question\nFollow the example we had in the classroom, find the \\(E(X)\\) and \\(Var(X)\\) based on (i) equations (2.1) and (2.2), and (ii) simulation for the following cases. Does (i) and (ii) give you the same result?\n(a) \\(X\\sim\\chi^2(\\nu=N+1)\\), where \\(N\\sim Poisson(\\lambda)\\).\n(b) (Only for Graduate Students) \\(X\\sim Binomial(n=Y, p=P)\\), where \\(Y\\sim Poisson(\\lambda)\\) and \\(P\\sim Beta(\\alpha, \\beta)\\).",
    "crumbs": [
      "HW",
      "HW 1"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#benchmarking-methods",
    "href": "slides/Chapter15.html#benchmarking-methods",
    "title": "Computational Statistics",
    "section": "Benchmarking Methods",
    "text": "Benchmarking Methods\n\nOverview of benchmarking in R\nExample: Benchmarking methods to generate a sequence\n\n\ns1 &lt;- 1:10\ns2 &lt;- seq(1, 10, 1)\ns3 &lt;- seq.int(1, 10, 1)\n\n# Benchmarking\nlibrary(microbenchmark)\nlibrary(ggplot2)\n\nn &lt;- 1000\nmb &lt;- microbenchmark(\n  seq(1, n, 1),\n  1:n,\n  times = 100\n)\nmb\n\nUnit: nanoseconds\n         expr   min    lq     mean  median      uq   max neval\n seq(1, n, 1) 17703 18224 22343.47 19737.0 24756.5 64511   100\n          1:n    90   110   200.13   185.5   231.0  1543   100\n\nautoplot(mb)",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#profiling-code-for-performance",
    "href": "slides/Chapter15.html#profiling-code-for-performance",
    "title": "Computational Statistics",
    "section": "Profiling Code for Performance",
    "text": "Profiling Code for Performance\n\nIntroduction to profiling in R\nExample: Profiling a function using profvis\n\n# Install profvis for profiling\nlibrary(profvis)\n\n# Example: Profile a sorting function\nprofvis({\n  x &lt;- rnorm(1e4)\n  y &lt;- sort(x)\n})",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#visualizing-profiling-results",
    "href": "slides/Chapter15.html#visualizing-profiling-results",
    "title": "Computational Statistics",
    "section": "Visualizing Profiling Results",
    "text": "Visualizing Profiling Results\n\n# profvis visual interface will show the profiling output",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter15.html#conclusion",
    "href": "slides/Chapter15.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of benchmarking and profiling in R\nPractice: Apply these techniques to optimize code in R\n\n\n\n\n\n🔗 tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Programming Topics"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#introduction-to-jackknife-after-bootstrap",
    "href": "slides/Chapter9.html#introduction-to-jackknife-after-bootstrap",
    "title": "Computational Statistics",
    "section": "Introduction to Jackknife-after-Bootstrap",
    "text": "Introduction to Jackknife-after-Bootstrap\n\nExplanation of the jackknife-after-bootstrap method\nUsed to estimate bias and standard errors after bootstrap\n\n\nlibrary(boot)\nlibrary(bootstrap)\nset.seed(1111)\n\n# Function to compute the patch ratio statistic\ntheta.boot &lt;- function(patch, i) {\n  y &lt;- patch[i, \"y\"]\n  z &lt;- patch[i, \"z\"]\n  mean(y) / mean(z)\n}\n\n# Bootstrap the patch dataset\nboot.out &lt;- boot(bootstrap::patch, statistic = theta.boot, R=2000)\nboot.out\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = bootstrap::patch, statistic = theta.boot, R = 2000)\n\n\nBootstrap Statistics :\n      original      bias    std. error\nt1* -0.0713061 0.008458915   0.1009777",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#applying-jackknife-after-bootstrap",
    "href": "slides/Chapter9.html#applying-jackknife-after-bootstrap",
    "title": "Computational Statistics",
    "section": "Applying Jackknife-after-Bootstrap",
    "text": "Applying Jackknife-after-Bootstrap\n\nExample: Using the jackknife-after-bootstrap method\nChecking the bootstrap array\n\n\n# Check the bootstrap array\nA &lt;- boot.array(boot.out)\nhead(A, 3)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    1    2    1    2    0    0    2\n[2,]    0    2    1    0    1    4    0    0\n[3,]    1    1    0    1    0    1    0    4\n\n# Proportion of cases that are not resampled\nmean(A[, 1] == 0)\n\n[1] 0.3455",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#visualizing-results",
    "href": "slides/Chapter9.html#visualizing-results",
    "title": "Computational Statistics",
    "section": "Visualizing Results",
    "text": "Visualizing Results\n\n# Plotting the bootstrap results\nplot(boot.out, index=1)",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter9.html#conclusion",
    "href": "slides/Chapter9.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of jackknife-after-bootstrap technique\nImportance of combining jackknife and bootstrap for robust estimates\nPractice: Apply jackknife-after-bootstrap to other datasets\n\n\n\n\n\n🔗 tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Jackknife-after-Bootstrap"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#introduction-to-monte-carlo-integration",
    "href": "slides/Chapter6.html#introduction-to-monte-carlo-integration",
    "title": "Computational Statistics",
    "section": "Introduction to Monte Carlo Integration",
    "text": "Introduction to Monte Carlo Integration\n\nBasic concepts of Monte Carlo integration\nExample: Integrating using uniform distribution\n\n\nm &lt;- 10000\nx &lt;- runif(m)\ntheta.hat &lt;- mean(exp(-x))\ntheta.hat\n\n[1] 0.6319788\n\n1 - exp(-1)\n\n[1] 0.6321206",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#example-monte-carlo-integration-with-bounded-intervals",
    "href": "slides/Chapter6.html#example-monte-carlo-integration-with-bounded-intervals",
    "title": "Computational Statistics",
    "section": "Example: Monte Carlo integration with bounded intervals",
    "text": "Example: Monte Carlo integration with bounded intervals\n\nm &lt;- 10000\nx &lt;- runif(m, min=2, max=4)\ntheta.hat &lt;- mean(exp(-x)) * 2\ntheta.hat\n\n[1] 0.1171909\n\nexp(-2) - exp(-4)\n\n[1] 0.1170196",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#unbounded-intervals",
    "href": "slides/Chapter6.html#unbounded-intervals",
    "title": "Computational Statistics",
    "section": "Unbounded Intervals",
    "text": "Unbounded Intervals\n\nHandling unbounded intervals in Monte Carlo integration\n\n\n# Plotting a function over an unbounded interval\nx &lt;- seq(.1, 2.5, length.out = 100)\ny &lt;- exp(-x^2 / 2)\nplot(x, y, type=\"l\", main=\"Function Plot for Unbounded Interval\", col=\"blue\")",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#introduction-to-importance-sampling",
    "href": "slides/Chapter6.html#introduction-to-importance-sampling",
    "title": "Computational Statistics",
    "section": "Introduction to Importance Sampling",
    "text": "Introduction to Importance Sampling\n\nImportance sampling and its applications\nExample: Applying importance sampling\n\n\n# Example of importance sampling\ng &lt;- function(x) exp(-x^2 / 2)\nf &lt;- function(x) dnorm(x, mean = 1)\nx &lt;- rnorm(10000, mean = 1)\ntheta.hat &lt;- mean(g(x) / f(x))\ntheta.hat\n\n[1] 2.551865",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#visualizing-importance-sampling",
    "href": "slides/Chapter6.html#visualizing-importance-sampling",
    "title": "Computational Statistics",
    "section": "Visualizing Importance Sampling",
    "text": "Visualizing Importance Sampling\n\n# Compare the original function with the importance sampling function\ncurve(g, from = -3, to = 3, col = \"blue\", lwd = 2, main=\"Importance Sampling: g(x) vs f(x)\")\ncurve(f, add = TRUE, col = \"red\", lty = 2, lwd = 2)\nlegend(\"topright\", legend=c(\"g(x)\", \"f(x)\"), col=c(\"blue\", \"red\"), lty=c(1, 2), lwd=2)",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#control-variates",
    "href": "slides/Chapter6.html#control-variates",
    "title": "Computational Statistics",
    "section": "Control Variates",
    "text": "Control Variates\n\nExplanation of control variates and how they reduce variance\nExample: Using control variates in Monte Carlo integration\n\n\n# Control variates example\nm &lt;- 10000\nx &lt;- runif(m)\ntheta.hat &lt;- mean(exp(-x)) - (mean(x) - 0.5)\ntheta.hat\n\n[1] 0.6344124",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter6.html#antithetic-variables",
    "href": "slides/Chapter6.html#antithetic-variables",
    "title": "Computational Statistics",
    "section": "Antithetic Variables",
    "text": "Antithetic Variables\n\nExplanation of antithetic variables and how they reduce variance\nExample: Applying antithetic variables\n\n\n# Antithetic variables example\nu &lt;- runif(5000)\ntheta.hat &lt;- mean((exp(-u) + exp(-(1 - u))) / 2)\ntheta.hat\n\n[1] 0.6320175\n\n1 - exp(-1)\n\n[1] 0.6321206",
    "crumbs": [
      "Slides",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#bootstrap-resampling-method",
    "href": "slides/Chapter8.html#bootstrap-resampling-method",
    "title": "Computational Statistics",
    "section": "Bootstrap Resampling Method",
    "text": "Bootstrap Resampling Method\n\nIntroduction to the bootstrap method\nExample: Bootstrap estimate of standard error\n\n\n# Bootstrap example: Estimate correlation between LSAT and GPA\nlibrary(bootstrap)    #for the law data\nprint(cor(law$LSAT, law$GPA))\n\n[1] 0.7763745\n\nprint(cor(law82$LSAT, law82$GPA))\n\n[1] 0.7599979\n\n# Set up the bootstrap\nB &lt;- 200            # number of replicates\nn &lt;- nrow(law)      # sample size\nR &lt;- numeric(B)     # storage for replicates\n# Bootstrap estimate of correlation\nfor (i in 1:B) {\n  idx &lt;- sample(1:n, size=n, replace=TRUE)\n  law_boot &lt;- law[idx, ]\n  R[i] &lt;- cor(law_boot$LSAT, law_boot$GPA)\n}\nmean(R)\n\n[1] 0.7611695\n\nsd(R)  # Bootstrap standard error\n\n[1] 0.1368807",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#visualizing-bootstrap-resamples",
    "href": "slides/Chapter8.html#visualizing-bootstrap-resamples",
    "title": "Computational Statistics",
    "section": "Visualizing Bootstrap Resamples",
    "text": "Visualizing Bootstrap Resamples\n\n# Plotting the bootstrap distribution\nhist(R, main=\"Bootstrap Distribution of Correlation\", col=\"lightblue\")",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#introduction-to-the-jackknife-method",
    "href": "slides/Chapter8.html#introduction-to-the-jackknife-method",
    "title": "Computational Statistics",
    "section": "Introduction to the Jackknife Method",
    "text": "Introduction to the Jackknife Method\n\nThe jackknife method for bias reduction\nExample: Jackknife estimate of the mean\n\n\n# Jackknife estimate of mean\nn &lt;- nrow(law)\ntheta_hat &lt;- mean(law$LSAT)\ntheta_jack &lt;- numeric(n)\n\n# Leave-one-out jackknife\nfor (i in 1:n) {\n  theta_jack[i] &lt;- mean(law$LSAT[-i])\n}\n\n# Jackknife estimate of bias\nbias_jack &lt;- (n - 1) * (mean(theta_jack) - theta_hat)\nbias_jack\n\n[1] 0",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#bootstrap-confidence-intervals",
    "href": "slides/Chapter8.html#bootstrap-confidence-intervals",
    "title": "Computational Statistics",
    "section": "Bootstrap Confidence Intervals",
    "text": "Bootstrap Confidence Intervals\n\nIntroduction to bootstrap confidence intervals\nExample: Bootstrap percentile confidence interval\n\n\n# Bootstrap confidence intervals\nalpha &lt;- 0.05\nci &lt;- quantile(R, probs = c(alpha/2, 1 - alpha/2))\nci  # Bootstrap confidence interval\n\n     2.5%     97.5% \n0.4756909 0.9494971",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter8.html#comparison-of-bootstrap-and-jackknife",
    "href": "slides/Chapter8.html#comparison-of-bootstrap-and-jackknife",
    "title": "Computational Statistics",
    "section": "Comparison of Bootstrap and Jackknife",
    "text": "Comparison of Bootstrap and Jackknife\n\nDiscuss the differences and use cases of bootstrap and jackknife\nRecap of methods and their benefits in reducing bias and estimating uncertainty",
    "crumbs": [
      "Slides",
      "Bootstrap and Jackknife"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#introduction-to-optimization",
    "href": "slides/Chapter14.html#introduction-to-optimization",
    "title": "Computational Statistics",
    "section": "Introduction to Optimization",
    "text": "Introduction to Optimization\n\nOverview of optimization methods\nExample: One-dimensional optimization using optimize\n\n\nf &lt;- function(x) log(x + log(x)) / log(1 + x)\n\n# Plotting the function\ncurve(f(x), from = 2, to = 15, ylab = \"f(x)\")\n\n# Finding the maximum\nres &lt;- optimize(f, lower = 4, upper = 8, maximum = TRUE)\nabline(v=res$maximum, col = \"red\", lwd = 2)\n\nres$maximum\n\n[1] 5.792299",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#introduction-to-mle",
    "href": "slides/Chapter14.html#introduction-to-mle",
    "title": "Computational Statistics",
    "section": "Introduction to MLE",
    "text": "Introduction to MLE\n\nIntroduction to Maximum Likelihood Estimation (MLE)\nExample: MLE for Gamma distribution\n\n\n# MLE for Gamma distribution\nm &lt;- 20000\nest &lt;- matrix(0, m, 2)\nn &lt;- 2000\nr &lt;- 5\nlambda &lt;- 2\n\nobj &lt;- function(lambda, xbar, logx.bar) {\n  r &lt;- length(xbar)\n  log(lambda) - lambda * mean(xbar) + logx.bar\n}",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#optimizing-mle",
    "href": "slides/Chapter14.html#optimizing-mle",
    "title": "Computational Statistics",
    "section": "Optimizing MLE",
    "text": "Optimizing MLE\n\n# Optimizing the MLE\nxbar &lt;- rnorm(n, mean = r/lambda, sd = 1)\nlogx.bar &lt;- mean(log(xbar))\nresult &lt;- optimize(obj, lower = 1, upper = 10, xbar = xbar, logx.bar = logx.bar, maximum = TRUE)\nresult\n\n$maximum\n[1] 9.999954\n\n$objective\n[1] NaN",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter14.html#conclusion",
    "href": "slides/Chapter14.html#conclusion",
    "title": "Computational Statistics",
    "section": "Conclusion",
    "text": "Conclusion\n\nRecap of one-dimensional optimization and MLE techniques\nPractice: Apply optimization to other statistical problems and models\n\n\n\n\n\n🔗 tinyurl.com/Comp-Stat",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/Chapter5.html#customizing-scatterplot-matrices",
    "href": "slides/Chapter5.html#customizing-scatterplot-matrices",
    "title": "Computational Statistics",
    "section": "Customizing Scatterplot Matrices",
    "text": "Customizing Scatterplot Matrices\n\nCustomizing scatterplot matrices with density plots\n\n\n# Adding density plots to scatterplot matrix\npanel.d &lt;- function(x, ...) {\n    usr &lt;- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(usr[1:2], 0, .5))\n    lines(density(x), col=\"red\")\n}\npairs(iris[101:150, 1:4], panel = panel.d, main=\"Customized Scatterplot Matrix\")",
    "crumbs": [
      "Slides",
      "Visualization"
    ]
  }
]